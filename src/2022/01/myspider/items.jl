{"href": "https://www.cnpython.com/qa/1473166", "title": "CMake使用自制公式安装pybind11绑定", "content": "<div class=\"show-content\"><p>我想为Cube项目提供一个自制程序公式，它用pypyd11编写和安装一个C++库及其Python绑定。理想情况下，该公式应通过运行普通</p><pre><code>cmake --build . --target install\n</code></pre><p>此安装流程在本地运行良好，但使用自制公式会在Python绑定的安装目录中引入一个问题：虽然头和库安装在由#{prefix}标识的Cell中的适当目录中，但绑定需要位于Python可见的site packages目录中。我正在使用CMake获取这样的目录</p><pre><code>install(TARGETS pyariadne DESTINATION ${Python_SITEARCH})\n</code></pre><p>但该目录似乎不能由自制编写，返回一个<code>Operation not permitted</code>。\n通过以下命令标识安装目录</p><pre><code>execute_process(COMMAND python3 -m site --user-site OUTPUT_VARIABLE INSTALL_DIR)\n</code></pre><p>也不起作用，因为自制在/tmp中标识临时用户站点，因此安装在那里的任何库随后都会被删除</p><p>我该如何在不改变目录权限的情况下安装从自制软件到其他软件？我希望避免为pypi打包，并使用pip单独安装绑定</p><p>编辑（输出示例，涉及的目录）：</p><pre><code>[109/110] Install the project...\n-- Install configuration: \"Release\"\n-- Installing: \n\n/usr/local/Cellar/python@3.9/3.9.2_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyariadne.so\nCMake Error at cmake_install.cmake:49 (file):\n  file INSTALL cannot copy file\n  \"/tmp/ariadne-20210305-1763-ggejxl/ariadne-2.1-rc2/build/pyariadne.so\" to\n  \n\"/usr/local/Cellar/python@3.9/3.9.2_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyariadne.so\":\n  Operation not permitted.\n</code></pre><p>目录<code>/usr/local/Cellar/python@3.9/3.9.2_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages</code>链接到<code>/usr/local/lib/python3.9/site-packages</code>。前者有用户lgeretti:staff，后者有用户lgeretti:admin</p><p>这个问题不仅发生在我的机器上，而且我还在macos:Lastgithub Actions机器上验证它，其中唯一需要的步骤是brew安装软件包</p></div>"}
{"href": "https://www.cnpython.com/qa/1473145", "title": "运行tf.distribute MultiWorkerMirroredStrategy时出现“地址已在使用”错误。如何使用端口终止进程", "content": "<div class=\"show-content\"><p>最近，我无法运行分布式。捕获的异常只是“无法启动gRPC服务器”</p><p>其他相关产出：</p><pre class=\"lang-json prettyprint-override\"><code>E0315 13:10:30.924933027    1721 server_chttp2.cc:40]        {\"created\":\"@1615839030.924848216\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc\",\"file_line\":395,\"referenced_errors\":[{\"created\":\"@1615839030.924842411\",\"description\":\"Failed to add any wildcard listeners\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc\",\"file_line\":342,\"referenced_errors\":[{\"created\":\"@1615839030.924819058\",\"description\":\"Unable to configure socket\",\"fd\":7,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":216,\"referenced_errors\":[{\"created\":\"@1615839030.924814323\",\"description\":\"Address already in use\",\"errno\":98,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":189,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]},{\"created\":\"@1615839030.924841231\",\"description\":\"Unable to configure socket\",\"fd\":7,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":216,\"referenced_errors\":[{\"created\":\"@1615839030.924837099\",\"description\":\"Address already in use\",\"errno\":98,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":189,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]}]}]}\n2021-03-15 13:10:30.925019: E tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:533] Unknown: Could not start gRPC server\n2021-03-15 13:10:30.925374: E tensorflow/c/c_api_experimental.cc:520] Could not start gRPC server\n</code></pre><p>“修复”是更改gRPC端口。但是，我想知道是什么进程锁定地址/端口并将其删除</p><p>如何识别和终止导致问题的进程</p><ul><li>我没有管理员权限李&gt;</li><li>我在多台主机上运行李&gt;</li><li>TensorFlow 2.4.1</li></ul></div>"}
{"href": "https://www.cnpython.com/qa/1473146", "title": "Python matplotlib Y轴标签乘以标量", "content": "<div class=\"show-content\"><p>我正在尝试从numpy.array绘制图像。Y轴标签是线性的，但我需要值乘以一个数字。在本例中，Y标签应在0到4000之间，因此乘以2</p><p><a href=\"https://i.stack.imgur.com/SBtXz.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/SBtXz.png\" alt=\"enter image description here\"></a></p><p>有什么想法吗</p><p>到目前为止，我的代码是：</p><pre><code>import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\n\nfig = plt.figure('Nav Panel')\nfig.set_size_inches(12, 12)\nimgplot = plt.imshow(nupyarray,interpolation='bicubic')\nimgplot.set_cmap('Greys_r')\nplt.colorbar()\nplt.show()\n</code></pre><p>谢谢</p></div>"}
{"href": "https://www.cnpython.com/qa/1473147", "title": "在同一触发器下运行多个Google云函数时，对BigQuery的请求过多", "content": "<div class=\"show-content\"><p>我猜这是一个非常简单的问题，但我不知道它的术语；希望这是一个快速的回答和结束</p><p>项目信息：这是一个基于Google云平台的Python 3.8项目，使用云函数、BigQuery、Secret Manager、PubSub、Scheduler，并使用服务帐户（不是项目默认）进行身份验证。前面提到的服务帐户对它所做的一切都有适当的权限-在单独触发时效果很好</p><p>我有几个谷歌云功能应用程序，我想在同一时间执行。我使用一个服务帐户在两个应用程序之间以及在同一个GCP项目中进行身份验证；让事情变得简单和美好，因为如果我想做另一个实例，我已经设置了很多东西。不幸的是，当我在另一个应用程序运行时执行其中一个应用程序时，初始应用程序将失败，可能是因为新应用程序上的服务帐户现在具有身份验证</p><p>是否有任何方法可以修复此问题，以便两个应用程序能够继续使用相同的凭据进行身份验证？这会像在Google secret Manager中使用不同的机密版本一样简单吗</p><p>通常我会推迟，但我主要是在寻找一种方法，使我的应用程序具有可扩展性，并且不会与正在运行的其他应用程序发生冲突</p><p><strong>更新：</strong>\n从日志来看，我的BigQuery函数的具体调用方式似乎存在问题。这似乎与配额有关，特别是与表更新和polling.py有关，但我不确定是什么原因造成的？看起来很不寻常</p><pre><code>    Traceback (most recent call last):\n      File \"/layers/google.python.pip/pip/lib/python3.8/site-packages/flask/app.py\", line 2447, in wsgi_app\n        response = self.full_dispatch_request()\n      File \"/layers/google.python.pip/pip/lib/python3.8/site-packages/flask/app.py\", line 1952, in full_dispatch_request\n        rv = self.handle_user_exception(e)\n      File \"/layers/google.python.pip/pip/lib/python3.8/site-packages/flask/app.py\", line 1821, in handle_user_exception\n        reraise(exc_type, exc_value, tb)\n      File \"/layers/google.python.pip/pip/lib/python3.8/site-packages/flask/_compat.py\", line 39, in reraise\n        raise value\n      File \"/layers/google.python.pip/pip/lib/python3.8/site-packages/flask/app.py\", line 1950, in full_dispatch_request\n        rv = self.dispatch_request()\n      File \"/layers/google.python.pip/pip/lib/python3.8/site-packages/flask/app.py\", line 1936, in dispatch_request\n        return self.view_functions[rule.endpoint](**req.view_args)\n      File \"/layers/google.python.pip/pip/lib/python3.8/site-packages/functions_framework/__init__.py\", line 149, in view_func\n        function(data, context)\n      File \"/workspace/main.py\", line 1016, in main\n        primaryRequest(ga_sessions_1g1, reportName='ga_sessions_1g1')\n      File \"/workspace/main.py\", line 1003, in primaryRequest\n        write_to_bq(reportName)\n      File \"/workspace/main.py\", line 958, in write_to_bq\n        job.result()  # Waits for the job to complete.\n      File \"/layers/google.python.pip/pip/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py\", line 631, in result\n        return super(_AsyncJob, self).result(timeout=timeout, **kwargs)\n      File \"/layers/google.python.pip/pip/lib/python3.8/site-packages/google/api_core/future/polling.py\", line 134, in result\n        raise self._exception\n    google.api_core.exceptions.Forbidden: 403 Exceeded rate limits: too many table update operations for this table. For more information, see https://cloud.google.com/bigquery/troubleshooting-errors\n</code></pre><p>下面是包含上述第958行的函数：</p><pre><code>    def write_to_bq(bqLookup):\n        client = bigquery.Client(credentials=scopedBqCredentials, project=bqCredentials.project_id,)\n        bqTableLookup = bqTableDict.get(bqLookup)\n        table_id = f'{gcpProject}.{bqDataset}.{bqTableLookup}'\n        job_config = bigquery.LoadJobConfig(\n            source_format=bigquery.SourceFormat.CSV, skip_leading_rows=0, autodetect=False,\n        )\n        with open('/tmp/result.csv', \"rb\") as source_file:\n            job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n        job.result()  # Waits for the job to complete.\n        table = client.get_table(table_id)  # Make an API request.\n        print(\"Loaded {} rows and {} columns to {}\".format(\n            table.num_rows, len(table.schema), table_id))\n</code></pre></div>"}
{"href": "https://www.cnpython.com/qa/1473141", "title": "无法导入使用'pip install e'安装的本地包`", "content": "<div class=\"show-content\"><p>我有一个正在开发的包，还有一些导入这个包的脚本。我正在尝试使用<code>pip install -e .</code>以开发人员模式安装软件包，但是无法在脚本中导入它。\n我有以下文件结构，其中<code>singlepixel</code>是我正在开发的一个包</p><pre><code>├── setup.py\n├── singlepixel\n│   ├── acquisition.py\n│   ├── metadata.py\n│   └── __init__.py\n├── scripts\n│   ├── script1.py\n│   └── script2.py\n</code></pre><p>My<code>__init__.py</code>具有以下结构：</p><pre><code>from .acquisition import init, setup, acquire, disconnect\nfrom .metadata import MetaData, AcquisitionParameters\n</code></pre><p>我的<code>setup.py</code>是：</p><pre><code>from setuptools import setup, find_packages\n\nsetup(\n    name='singlepixel',\n    version='0.0.1',\n    author='gbm',\n    package_dir={\"\": \"singlepixel\"},\n    packages=find_packages(where=\"singlepixel\"))\n</code></pre><p>根据我的理解，我应该能够在我的<code>script1.py</code>中执行一个简单的import语句，例如<code>from singlepixel import *</code>，但是它不起作用，而是出现以下错误：</p><pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nModuleNotFoundError: No module named 'singlepixel'\n</code></pre><p>此外，即使我使用的是Python环境，我似乎也无法在任何地方导入包。据我所知，在使用此环境时，我应该能够在任何地方导入我的包，就像使用<code>pip install</code>安装的任何其他包一样</p></div>"}
{"href": "https://www.cnpython.com/qa/1473139", "title": "我可以控制在SqlAlchemy连接池中进行预处理连接检查的频率吗？", "content": "<div class=\"show-content\"><h2>环境/使用案例信息</h2><p>我有一个SqlAlchemy django-postgrespool2连接池，用于连接到PostgreSQL数据库</p><h2>问题</h2><p>我正在考虑在我的SqlAlchemy连接池中使用预ping功能。但是，我了解到，每次池连接签出都会进行预ping检查。我已经做了一些测试，对我来说，对于发送到数据库的每个SQL命令，都会发生这种签出。我担心，如果对发送到数据库的每个SQL命令都进行检查，那么所有这些预ping检查都会导致很大的开销/延迟</p><h2>问题:</h2><p>是否有办法配置应进行预ping检查的频率或时间？或者我必须实现自己的解决方案来定制检查连接的频率？怎么做</p><p>谢谢</p></div>"}
{"href": "https://www.cnpython.com/qa/1473143", "title": "无法通过USB串行python库与数字千分表通信", "content": "<div class=\"show-content\"><p>我有一个数字千分表：Helios Preisser Digimet 1722-502”。它具有通过USB串行电缆输出读数的能力。USB电缆一端是一个特殊的4针连接器，插入卡钳，另一端是一个普通USB</p><p>尽管该设备带有特殊的软件，但我正在尝试编写一个基本的python库来与之通信\n<a href=\"https://i.stack.imgur.com/jKPwD.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/jKPwD.png\" alt=\"\"></a></p><p>我正在使用python串行库，并设法与它进行一些通信</p><pre><code>import serial\nser = serial.Serial(port ='/dev/tty.usbserial-MA4LOCLF', baudrate=4800,parity=serial.PARITY_EVEN,  bytesize=serial.SEVENBITS,stopbits=serial.STOPBITS_TWO, dsrdtr=True, xonxoff=True)\n# press the small red button on the cable. This generates a data entry\nIn [77]: ser.inWaiting()\nOut[77]: 8\nIn [78]: ser.read(8)\nOut[78]: '+000.00\\r'\n</code></pre><p>因此，当使用该模式时，按下插入千分表的电缆上的红色小按钮请求数据时，这种方式非常有效</p><p>但是，还有另一种模式，用户可以请求数据输入。这是手册中描述的“通过外围设备请求进行数据传输”模式，用户必须为T1（100ms&amp;lt；T1&amp;lt；1000ms）低脉冲数据请求引脚。\n我几乎是随机地尝试了所有可能的组合，我认为可以让这个数据请求工作，但没有结果。所有尝试使用串行库中的写函数都没有工作</p><pre><code>In [79]: ser.write('0\\r')\nOut[79]: 2\n\nIn [80]: ser.inWaiting()\nOut[80]: 0\n</code></pre><p>我有点不知所措。我知道这种模式是有效的，因为当你下载（仅在Windows中）设备附带的软件时，你有能力发送该请求。因此必须有一种方法可以用python串行库模拟该请求，但我被卡住了，我甚至不知道如何继续。\n任何帮助都将不胜感激。\n谢谢</p></div>"}
{"href": "https://www.cnpython.com/qa/1473138", "title": "使用tablapy从PDF中读取表格和文本", "content": "<div class=\"show-content\"><p>是否可以使用tabla-py从PDF中同时提取表格和文本</p><p>PDF的顶部如下所示：\n<a href=\"https://i.stack.imgur.com/FKEVD.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/FKEVD.jpg\" alt=\"enter image description here\"></a></p><p>使用此代码：</p><pre><code>import tabula as tab\n\ndf = tab.read_pdf('https://www.banque-france.fr/sites/default/files/pspp_liste_isn_en-20160725.pdf',pages='all')\n</code></pre><p>它在获取表格方面做得很好，但我还想获取文本行，“上次更新：2016年7月25日”</p><p>有没有一种方法可以让tabla-py在一个过程中为我做到这一点，或者我需要两次解析文件，一次使用tabla-py，第二次使用另一个包来读取文本？我尝试了lattice=和stream=的不同组合，但没有成功</p><p>（虽然在我的示例中，日期也是链接的一部分，但情况并非总是如此，因此我不能依赖于此，需要从PDF中获取日期）</p></div>"}
{"href": "https://www.cnpython.com/qa/1473144", "title": "如果字符串包含子字符串列表中的任何元素，如何查找该字符串的起始索引", "content": "<div class=\"show-content\"><p>假设下面有一个子字符串列表：</p><pre><code>my_list = [\"am\", \"is\", \"are\"]\n</code></pre><p>我想在字符串中搜索此列表的元素。如果字符串包含列表中的任何项，则应打印字符串中此子字符串的起始索引</p><p>字符串为：</p><pre><code>s = \"I am a Python developer.\"\n</code></pre><p>很明显，字符串包含<code>\"am\"</code>，并且字符串中此子字符串的起始索引为<code>2</code></p><p>有一次我想用：</p><pre><code>if \"am\" in s:\n    print(s.find(\"am\"))\n</code></pre><p>但我仅通过列表的一个元素限制了搜索操作。字符串中最多可以有列表中的一项</p></div>"}
{"href": "https://www.cnpython.com/qa/1473148", "title": "使用归一化迭代计数的Mandelbrot集", "content": "<div class=\"show-content\"><p>我有以下Python程序，该程序试图使用<a href=\"https://en.wikipedia.org/wiki/Plotting_algorithms_for_the_Mandelbrot_set#Continuous_(smooth)_coloring\" rel=\"nofollow noreferrer\">normalized iteration count algorithm</a>为Mandelbrot集着色：</p><pre class=\"lang-py prettyprint-override\"><code>from PIL import Image\nimport numpy as np\nfrom matplotlib.colors import hsv_to_rgb\n\nsteps = 256 # maximum iterations\nbailout_radius = 64 # bailout radius\n\ndef normalized_iteration(n, abs_z):\n    return n + 1 - np.log2(np.log(abs_z))/np.log(2)\n\ndef make_set(real_start, real_end, imag_start, imag_end, height):\n\n    width = \\\n        int(abs(height * (real_end - real_start) / (imag_end - imag_start)))\n\n    real_axis = \\\n        np.linspace(real_start, real_end, num = width)\n    imag_axis = \\\n        np.linspace(imag_start, imag_end, num = height)\n    complex_plane = \\\n        np.zeros((height, width), dtype = np.complex_)\n\n    real, imag = np.meshgrid(real_axis, imag_axis)\n\n    complex_plane.real = real\n    complex_plane.imag = imag\n\n    pixels = \\\n        np.zeros((height, width, 3), dtype = np.float_)\n\n    new = np.zeros_like(complex_plane)\n    is_not_done = np.ones((height, width), dtype = bool)\n\n    # cosine_interpolation = lambda x: (np.cos(x * np.pi + np.pi) + 1) / 2\n    \n    for i in range(steps):\n        new[is_not_done] = \\\n            new[is_not_done] ** 2 + complex_plane[is_not_done]\n        \n        mask = np.logical_and(np.absolute(new) &gt; bailout_radius, is_not_done)\n        pixels[mask, :] = (i, 0.6, 1)\n        is_not_done = np.logical_and(is_not_done, np.logical_not(mask))\n\n    new_after_mask = np.zeros_like(complex_plane)\n    new_after_mask[np.logical_not(is_not_done)] = \\\n        new[np.logical_not(is_not_done)]\n    new_after_mask[is_not_done] = bailout_radius\n\n    pixels[:, :, 0] = \\\n        normalized_iteration(pixels[:, :, 0], np.absolute(new_after_mask)) / steps\n\n    image = Image.fromarray((hsv_to_rgb(np.flipud(pixels)) * 255).astype(np.uint8))\n\n    image.show()\n\nmake_set(-2, 1, -1, 1, 2000) \n</code></pre><p>它产生了一个相当好的图像。然而，当我将其与使用此算法的其他集合进行比较时，集合中的颜色几乎没有变化。如果我减少<code>steps</code>，我会得到一个变化更大的梯度，但这会降低分形的质量。这段代码的重要部分是我的<code>normalized_iteration</code>定义，它与<a href=\"https://en.wikipedia.org/wiki/Plotting_algorithms_for_the_Mandelbrot_set#Continuous_(smooth)_coloring\" rel=\"nofollow noreferrer\">this Wikipedia article's version,</a>略有不同</p><pre class=\"lang-py prettyprint-override\"><code>def normalized_iteration(n, abs_z):\n    return n + 1 - np.log2(np.log(abs_z))/np.log(2)\n</code></pre><p>其中我使用了该定义（将函数映射到像素数组）</p><pre class=\"lang-py prettyprint-override\"><code>pixels[:, :, 0] = \\\n        normalized_iteration(pixels[:, :, 0], np.absolute(new_after_mask)) / steps\n</code></pre><p>最后一个数组，我将HSV格式转换为RGB，并将[0,1]上的像素值转换为[0,255）上的值</p><pre class=\"lang-py prettyprint-override\"><code>image = Image.fromarray((hsv_to_rgb(np.flipud(pixels)) * 255).astype(np.uint8))\n</code></pre><p>我已经和这个问题斗争了一段时间了，我不确定到底出了什么问题。感谢你帮助我确定如何使渐变颜色更加多样化，以及如何适应我可能难以阅读的代码。此外，我意识到这里面还有优化的空间</p></div>"}
{"href": "https://www.cnpython.com/qa/1473142", "title": "Tensorflow数据API:repeat（）", "content": "<div class=\"show-content\"><p>下面的代码是“使用弯刀学习、Keras和tensorflow进行机器学习”的一个片段。\n我理解下面代码中的所有内容，除了第二行中的<code>.repeat(repeat)</code>函数链接</p><p>我知道repeat是重复数据集元素（即，在本例中是文件路径），如果参数设置为<code>None</code>或留空，则重复将永远持续，直到使用它的函数决定何时停止为止</p><p>正如您在下面的代码中所看到的，作者正在将<code>repeat()</code>参数设置为<code>None</code></p><p>1-基本上我想知道作者为什么决定这样做</p><p>2-或者是因为代码试图模拟数据集不适合内存的情况，如果是这种情况，那么在实际情况下，我们应该避免<code>repeat()</code>，对吗</p><pre class=\"lang-py prettyprint-override\"><code>\ndef csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n                       n_read_threads=None, shuffle_buffer_size=10000,\n                       n_parse_threads=5, batch_size=32):\n    dataset = tf.data.Dataset.list_files(filepaths, seed = 42).repeat(repeat)\n    dataset = dataset.interleave(\n        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n        cycle_length = n_readers, num_parallel_calls = n_read_threads)\n    \n    dataset = dataset.shuffle(shuffle_buffer_size)\n    dataset = dataset.map(preprocess, num_parallel_calls = n_parse_threads)\n    dataset = dataset.batch(batch_size)\n    return dataset.prefetch(1)\n\ntrain_set = csv_reader_dataset(train_filepaths, repeat = None)\nvalid_set = csv_reader_dataset(valid_filepaths)\ntest_set = csv_reader_dataset(test_filepaths)\n\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.InputLayer(input_shape = X_train.shape[-1: ]),\n    keras.layers.Dense(30, activation = 'relu'),\n    keras.layers.Dense(1)\n])\n\nm_loss = keras.losses.mean_squared_error\nm_optimizer = keras.optimizers.SGD(lr = 1e-3)\n\nbatch_size = 32\nmodel.compile(loss = m_loss, optimizer = m_optimizer, metrics = ['accuracy'])\nmodel.fit(train_set, steps_per_epoch = len(X_train) // batch_size, epochs = 10, validation_data = valid_set)\n</code></pre></div>"}
{"href": "https://www.cnpython.com/qa/1473140", "title": "如何在xaxis和yaxis上绘制matplotlib箭袋箭头？python", "content": "<div class=\"show-content\"><p>目前，我用matplotlib绘制的图如图1所示，正如您在x轴上看到的，我的箭头的一半被该轴覆盖</p><p>我希望整个箭头如图2所示</p><p>这是我代码的相关部分，X_fine和Y_fine的范围与X和Y相同，但有更多的垃圾箱</p><pre><code>X, Y = np.meshgrid(arrow_csf_centre, arrow_precip_centre)\nX_fine, Y_fine = np.meshgrid(arrow_csf_centre_fine, arrow_precip_centre_fine)\n\nplt.contourf(X_fine, Y_fine, np.transpose(counts2_fine), 30)\nplt.quiver(X,\n           Y,\n           np.transpose(arrow_dirs[:,:,0]),\n           np.transpose(arrow_dirs[:,:,1]), \n           angles='xy', \n           scale_units='xy',\n           scale=1,\n           pivot='mid')\n</code></pre><p>谢谢你的阅读</p><p><strong>图1-我的情节</strong><a href=\"https://i.stack.imgur.com/ZFcBt.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/ZFcBt.png\" alt=\"Figure 1 - my plot\"></a></p><p><strong>图2-目标图\n<a href=\"https://i.stack.imgur.com/Vc7jW.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/Vc7jW.png\" alt=\"enter image description here\"></a></strong></p></div>"}
{"href": "https://www.cnpython.com/qa/1473149", "title": "Python3在使用“set”时如何比较浮动？", "content": "<div class=\"show-content\"><div></div><p>Python中的<code>set</code>函数依赖于<code>set</code>元素上是否存在健壮的相等运算符</p><p>假设我们有一个浮点数列表，<code>xs</code>，我们称之为<code>set(xs)</code>。<code>set</code>将如何比较<code>xs</code>的元素？它会检查机器epsilon中是否有两个元素，还是执行位相等</p></div>"}
{"href": "https://www.cnpython.com/qa/1473150", "title": "GoogleDataProcPresto：如何使用Python运行查询", "content": "<div class=\"show-content\"><p>我已经通过本<a href=\"https://cloud.google.com/dataproc/docs/tutorials/presto-dataproc\" rel=\"nofollow noreferrer\">link</a>中的步骤建立了一个运行Presto的GoogleDataProc集群</p><p>它工作正常，我可以通过gcloud命令行工具运行查询，如下面的链接所示</p><pre class=\"lang-sh prettyprint-override\"><code>gcloud dataproc jobs submit hive \\\n    --cluster presto-cluster \\\n    --region=${REGION} \\\n    --execute \"SELECT COUNT(*) FROM chicago_taxi_trips_parquet;\"\n</code></pre><p>最后，本教程展示了如何通过java应用程序在Presto上运行查询。我试图用Python找到类似的解决方案。有没有办法通过Python应用程序在Dataproc集群上运行查询</p><p>我知道有用于Presto的Python客户端，但我无法找到有关如何将其与Dataproc集群上运行的Presto连接的资源</p><p>类似地，有一个Python库可以向Dataproc提交作业，但是没有关于如何向Dataproc集群提交Presto查询作业的参考资料</p><p>有人能告诉我，我们如何连接到Google Dataproc上的Presto并使用Python应用程序远程运行查询</p></div>"}
{"href": "https://www.cnpython.com/qa/1473151", "title": "汇总文本文件中相同键的值", "content": "<div class=\"show-content\"><p>我正试图写一个软件，总结从第三方文本文件的点。这是文本文件的外观：</p><pre><code>essi 5\npietari 9\nessi 2\npietari 10\npietari 7\naps 25\nessi 1\n</code></pre><p>主要功能是将每个玩家的得分总和返回到一个列表中，玩家按字母顺序排列。我已经做了所有的事情，除了能够计算数字的总和，它给了我文本文件中埃西和皮埃塔里的最后一个数字。这是我的密码：</p><pre><code>def main():\n\n    filename =  input(\"Enter the name of the score file: \")\n    read_file = open(filename, mode=\"r\")\n\n    score = {}\n    for line in read_file:\n        line = line.rstrip()\n        name, points = line.split(\" \")\n        score[name] = points\n\n    print(\"Contestant score:\")\n\n    for key in sorted(score):\n        print(key,score[key])\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre><p>它给出了：</p><pre><code>Enter the name of the score file: game.txt\nContestant score:\naps 25\nessi 1\npietari 7\n\nProcess finished with exit code 0\n</code></pre><p>所以基本上我需要的结果是：</p><pre><code>Enter the name of the score file: game.txt\nContestant score:\naps 25\nessi 8\npietari 26\n\nProcess finished with exit code 0\n</code></pre></div>"}
{"href": "https://www.cnpython.com/qa/1473152", "title": "Django Q筛选器，无法在单个查询中获得结果", "content": "<div class=\"show-content\"><p>我想使用多个条件进行查询，以从订单模型中获取一些对象，但我找不到一种方法在单个查询中获取所有结果。除了进行2次查询的选项外，我还想知道是否可以只进行一次查询，这样我就可以创建一个包含所有这些订单的CSV表（这部分已经起作用了，所以我将坚持查询）。\n条件：</p><p>支付方式：贝宝和莫莉<br>\n创建时间：15:00和16:00</p><p>&amp;amp</p><p>付款方式：ApplePay<br>\n创建时间：17:00和18:00<br></p><p>两个问题：</p><pre><code>Order.objects.all() \\\n.filter(Q(paymethod=\"Paypal\") | \n        Q(paymethod=\"Mollie\") &amp; \n        Q(created_at__hour__in=(15, 16)))\n\n\nOrder.objects.all() \\\n.filter(Q(paymethod=\"ApplePay\") \n        Q(created_at__hour__in=(17, 18)))\n</code></pre><p>这两个查询单独工作很好，但我想知道是否可以将它们组合到一个查询中</p><p>我试过这样的方法：</p><pre><code>Order.objects.all() \\\n.filter(Q(paymethod=\"Paypal\" | \"Mollie\", created_at__hour__in=(15, 16)) \\\n&amp; Q(paymethod=\"ApplePay\", created_at__hour__in=(17, 18)))\n</code></pre><p>上述操作不起作用，原因是：TypeError:“str”和“str”的操作数类型不受支持。\n因此，我没有<code>paymethod=\"Paypal\" | \"Mollie\"</code>，而是尝试：<code>paymethod=\"Paypal\" | paymethod=\"Mollie\"</code>\n但不幸的是，这也不起作用</p><p>如果有人能给我指出正确的方向，我将不胜感激。我还在学习django，django Q对我来说是新的。如果需要任何其他信息，请让我知道！谢谢</p></div>"}
{"href": "https://www.cnpython.com/qa/1473120", "title": "discord.py无当前事件循环", "content": "<div class=\"show-content\"><p>我有这个代码，理论上，它将检查一个通道中发送了多少条消息，并将在每天的某个特定时间执行此操作。代码如下：</p><pre><code>def checkTime():\n    # This function runs periodically every 1 second\n    threading.Timer(1, checkTime).start()\n\n    now = datetime.now()\n\n    current_time = now.strftime(\"%H:%M:%S\")\n    print(\"Current Time =\", current_time)\n\n    if(current_time == '21:35:20'):  # check if matches with the desired time\n        print(\"starting\")\n        channel = bot.get_channel(this is where I put the channel ID, just hiding it in here)\n        counter = 0\n        for message in channel.history():\n            counter += 1\n        print(counter)\n\n\ncheckTime()\n</code></pre><p>但是，一旦时间到达21:35:20，将显示以下错误消息：</p><pre><code>starting\nException in thread Thread-9:\nTraceback (most recent call last):\n  File \"C:\\Users\\jackt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 954, in _bootstrap_inner\n    self.run()\n  File \"C:\\Users\\jackt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 1266, in run\n    self.function(*self.args, **self.kwargs)\n  File \"C:\\Users\\jackt\\Desktop\\bot.py\", line 51, in checkTime\n    for message in channel.history():\n  File \"C:\\Users\\jackt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\discord\\abc.py\", line 1087, in history\n    return HistoryIterator(self, limit=limit, before=before, after=after, around=around, oldest_first=oldest_first)\n  File \"C:\\Users\\jackt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\discord\\iterators.py\", line 256, in __init__\n    self.messages = asyncio.Queue()\n  File \"C:\\Users\\jackt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\queues.py\", line 35, in __init__\n    self._loop = events.get_event_loop()\n  File \"C:\\Users\\jackt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 642, in get_event_loop\n    raise RuntimeError('There is no current event loop in thread %r.'\nRuntimeError: There is no current event loop in thread 'Thread-9'.\n</code></pre><p>我正在使用“从日期时间导入日期时间”和“导入线程”</p><p>请让我知道我的代码应该是什么样子来防止这个错误。谢谢大家!</p></div>"}
{"href": "https://www.cnpython.com/qa/1473117", "title": "从json文件传递mongodb查询以在python中执行", "content": "<div class=\"show-content\"><p>我有一个包含大约50个查询的<code>json</code>文件。\n<code>json</code>文件如下所示：</p><pre><code>{\n\n   \"1\": {\n     \"mongodb\":\"mydb1.mongodbtime.find({\\n    \\\"timestamp1\\\": {\\\"$gte\\\": datetime.strptime(\\\"2010-01-01 00:05:00\\\", \\\"%Y-%m-%d %H:%M:%S\\\"),\\n                   \\\"$lte\\\": datetime.strptime(\\\"2015-01-02 00:05:00\\\", \\\"%Y-%m-%d %H:%M:%S\\\")}},\\n    {\\\"id13\\\":1}),\",\n     \"mongodb1index\":\"mydb1.mongodbindextimestamp1.find({\\n    \\\"timestamp1\\\": {\\\"$gte\\\": datetime.strptime(\\\"2010-01-01 00:05:00\\\", \\\"%Y-%m-%d %H:%M:%S\\\"),\\n                   \\\"$lte\\\": datetime.strptime(\\\"2015-01-02 00:05:00\\\", \\\"%Y-%m-%d %H:%M:%S\\\")}},\\n    {\\\"id13\\\":1}),\"\n   },\n   \"2\": {\n     \"mongodb\":\"mydb1.mongodbtime.find({\\n    \\\"timestamp1\\\": {\\\"$gte\\\": datetime.strptime(\\\"2010-01-01 00:05:00\\\", \\\"%Y-%m-%d %H:%M:%S\\\"),\\n                   \\\"$lte\\\": datetime.strptime(\\\"2015-01-02 00:05:00\\\", \\\"%Y-%m-%d %H:%M:%S\\\")}},\\n    {\\\"id13\\\":1}),\",\n     \"mongodb1index\":\"mydb1.mongodbindextimestamp1.find({\\n    \\\"timestamp1\\\": {\\\"$gte\\\": datetime.strptime(\\\"2010-01-01 00:05:00\\\", \\\"%Y-%m-%d %H:%M:%S\\\"),\\n                   \\\"$lte\\\": datetime.strptime(\\\"2015-01-02 00:05:00\\\", \\\"%Y-%m-%d %H:%M:%S\\\")}},\\n    {\\\"id13\\\":1}),\n\n   }\n\n}\n\n</code></pre><p>我在数据库<code>mongodbtime</code>中有两个<code>collections</code>，一个名为<code>mongodbtime</code>，另一个名为<code>mongodbindextimestamp1</code>。\n我在python中用于传递<code>query</code>并执行它的代码如下所示：</p><pre><code>mydb1 = myclient[\"mongodbtime\"]\n\n\nwith open(\"queriesdb.json\",'r') as fp:\n    queries = json.load(fp)\n    db = {\"mongodb\": \"mongodbtime\", \"mongodb1index\": \"mongodbtime\"}\n    for num_query in queries.keys():\n        query = queries[\"1\"]\n        print(query)\n        for db_name in db:\n            print(db_name)\n            run(query[db_name])\n</code></pre><pre><code>def run(query):\n\n        for j in range(0, 1):\n            \n            start_time = time.time()\n            cursor = query\n            for x in cursor:\n                pprint(x)\n\n            # capture end time\n            end_time = time.time()\n            # calculate elapsed time\n            elapsed_time = end_time - start_time\n            times.append(elapsed_time)\n            #elapsed_time_milliSeconds = elapsed_time * 1000\n            #print(\"code elapsed time in milliseconds is \", elapsed_time_milliSeconds)\n        finalmeasurments(times)\n\n</code></pre><p>我像字符串一样传递它，显然当我<code>print(cursor)</code>它只是<code>print</code>我应该使用另一种形式的文件吗？\n你知道我应该如何执行我的<code>query</code>吗</p></div>"}
{"href": "https://www.cnpython.com/qa/1473122", "title": "缩进到括号在VS代码中的Jupyter笔记本中不起作用", "content": "<div class=\"show-content\"><p><a href=\"https://i.stack.imgur.com/kPszD.png\" rel=\"nofollow noreferrer\">Visual explanation</a></p><p>缩进到括号功能在Visual Studio代码上的Jupyter笔记本中不起作用。像Python缩进这样的扩展在Jupyter笔记本上不起作用。有人知道怎么解决吗</p></div>"}
{"href": "https://www.cnpython.com/qa/1473118", "title": "如何使用Flask创建“utils.py”文件", "content": "<div class=\"show-content\"><p>我希望你做得很好</p><p>我是Flask的新手，我正在尝试使用这个框架和SQLite作为数据库来构建我的第一个网站。我也在使用应用程序工厂模式</p><p>我正在考虑创建一个utils文件，将重复代码放在其中，我需要在该文件中包含的一个非常简单的函数处理数据库</p><p>详情如下:</p><p><strong>utils.py</strong></p><pre><code>def get_categories():\n    tuple_list = []\n    categories = Category.query.all()\n    for category in categories:\n        new_tuple = (category.id, category.name)\n        tuple_list.append(new_tuple)\n    return tuple_list\n</code></pre><p>正如您所看到的，这个函数非常简单，它只是从数据库中获取类别，并以我想要的格式返回它们，一个元组列表</p><p>现在的问题是，当我使用python run.py运行服务器时，会出现以下错误：</p><pre><code>RuntimeError: No application found. Either work inside a view function or push an application context. See http://flask-sqlalchemy.pocoo.org/contexts/.\n</code></pre><p>我知道为什么会这样，需要一个应用程序上下文来对数据库执行操作。但是，我无法导入我的“app”对象，因为它会给我一个“循环导入错误”</p><p>如果我这样做：</p><pre><code>def get_categories():\n    tuple_list = []\n    with app.app_context():\n    categories = Category.query.all()\n    for category in categories:\n        new_tuple = (category.id, category.name)\n        tuple_list.append(new_tuple)\nreturn tuple_list\n</code></pre><p>我明白了：</p><pre><code>ImportError: cannot import name 'posts' from partially initialized module 'soyciro.posts.routes' (most likely due to a circular import) (C:\\Users\\Ciro\\Documents\\projects\\python\\soyciro\\soyciro\\posts\\routes.py)\n</code></pre><p>我的应用程序目录中有一个<strong><strong>init</strong>.py</strong>文件，如下所示：</p><pre><code>from flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nfrom soyciro.config import Config\n\nfrom flask_bcrypt import Bcrypt\nfrom flask_login import LoginManager\n\ndb = SQLAlchemy()\nbcrypt = Bcrypt()\nlogin_manager = LoginManager()\nlogin_manager.login_view = 'users.login'\nlogin_manager.login_message_category = 'info'\n\n\ndef create_app(config_class=Config):\n    app = Flask(__name__)\n    app.config.from_object(Config)\n\n    db.init_app(app)\n    bcrypt.init_app(app)\n    login_manager.init_app(app)\n\n    from soyciro.main.routes import main\n    from soyciro.posts.routes import posts\n    from soyciro.work.routes import work\n    from soyciro.users.routes import users\n    from soyciro.categories.routes import categories\n    app.register_blueprint(main)\n    app.register_blueprint(posts)\n    app.register_blueprint(work)\n    app.register_blueprint(users)\n    app.register_blueprint(categories)\n\n    return app\n</code></pre><p>我在应用程序的根目录中还有一个<strong>run.py</strong>文件，其中包含以下内容：</p><pre><code>from myapp import create_app\n\napp = create_app()\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre><p>如果你需要更多的信息，请告诉我</p><p>提前感谢,，\n西罗</p></div>"}
{"href": "https://www.cnpython.com/qa/1473119", "title": "python中的`\\uuu插槽`的静态类型", "content": "<div class=\"show-content\"><p>我知道python使用了duck类型，但我想知道是否有可能对类变量执行类型验证，尤其是<code>__slots__</code></p><p>例如—</p><pre><code>class Student:\ndef __init__(self, name):\n    self.name = name\n\nclass Class:\n    __slots__ = ('class_representative',\n                 'var_2',\n                 'var_3',\n                 '...',  # Assume many more variables below\n                )\n\n    def __init__(self, *args, **kwargs):\n        self.class_representative = kwargs.get('cr')\n        self.var_2 = kwargs.get('v2')\n        self.var_3 = kwargs.get('v3')\n        ... # Assume many more variables below\n</code></pre><p>在上面的示例中，我如何确保无论何时将任何对象分配给<code>class_representative</code>变量，它都应该是<code>Student</code>类型</p><p>有可能出现以下情况吗</p><pre><code>class Class:\n    __slots__ = ('class_representative': Student,\n                 'var_2',\n                 'var_3',\n                 '...',  # Assume many more variables below\n                )\n</code></pre></div>"}
{"href": "https://www.cnpython.com/qa/1473121", "title": "使用Chrome驱动程序未捕获引用时出错错误：未定义showDate", "content": "<div class=\"show-content\"><p>我使用的是chrome驱动程序版本<code>89.0.4389.23</code>，我的chrome浏览器版本是<code>89.0.4389.90</code>。我有一个完美运行的代码，如下所示：</p><pre><code>from selenium.webdriver.chrome.options import Options\nfrom selenium import webdriver\n\noptions = Options()\noptions.add_argument('--headless')\noptions.add_argument('--disable-gpu')\ndriver = webdriver.Chrome(options=options)\ndriver.get('https://www.nccpl.com.pk/en/market-information/fipi-lipi/fipi-sector-wise') \n</code></pre><p>在上面提到的最后一行代码中，我遇到了一个错误<code>\"Uncaught ReferenceError: showDate is not defined\", source: https://www.nccpl.com.pk/en/portfolio-investments/fipi-sector-wise (1652)</code>，我无法解释这个错误并找出错误所在</p><p>我正在寻找对此错误的解释，以及如何修复它</p></div>"}
{"href": "https://www.cnpython.com/qa/1473123", "title": "使用set_data（）更新一行后，Matplotlib的“重置原始视图”按钮将缩放到以前数据的限制", "content": "<div class=\"show-content\"><p>使用Python 3.8、Matplotlib 3.3.2，在GTK3 GUI中嵌入图形画布</p><p><strong>问题陈述：</strong></p><p>我有一个带有GTK3 GUI的程序，下拉菜单中有数百个变量，每个变量都有要绘制的x/y数据。我正在尝试在我的图下方添加导航工具栏，以便用户可以放大和缩小。这适用于初始图形，但当用户选择不同的变量，并且使用<code>axis.lines[0].set_data(x,y)</code>更新行的数据时，工具栏上的<strong>重置原始视图</strong>按钮重置为第一个数据集的边界，而不是新的数据集（假设用户放大了第一个数据集）</p><p>有没有办法告诉NavigationToolbar图形中的打印数据已更改，以便重置视图可以正常工作</p><p><strong>示例代码：</strong></p><pre><code>import gi\ngi.require_version('Gtk', '3.0')\nfrom gi.repository import Gtk\n\nfrom matplotlib.backends.backend_gtk3agg import FigureCanvasGTK3Agg as FigureCanvas\nfrom matplotlib.backends.backend_gtk3 import NavigationToolbar2GTK3 as NavigationToolbar\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nclass PlotDemo(Gtk.Window):\n   def __init__(self):\n      Gtk.Window.__init__(self, title=\"Navigation Demo\")\n      self.set_border_width(4)\n\n      self.set_default_size(850, 650)\n\n      self.box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL)\n      self.add(self.box)\n\n      buttonBox = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL)\n      self.box.add(buttonBox)\n\n      sineButton = Gtk.Button(label='SIN')\n      sineButton.connect('clicked', self.plotSin)\n      buttonBox.add(sineButton)\n\n      cosineButton = Gtk.Button(label='COS')\n      cosineButton.connect('clicked', self.plotCos)\n      buttonBox.add(cosineButton)\n\n      self.figure, self.axis = plt.subplots()\n\n      self.time = np.arange(0.0, 3.0, 0.01)\n      self.sin = 100*np.sin(2*np.pi*self.time)\n      self.cos = np.cos(2*np.pi*self.time)\n\n      self.axis.plot(self.time, self.sin)\n      self.axis.set_xlim(0, 3)\n\n      self.canvas = FigureCanvas(self.figure)\n      self.canvas.set_size_request(800, 600)\n      self.canvas.draw()\n\n      self.navigation = NavigationToolbar(self.canvas, self)\n\n      self.box.add(self.canvas)\n      self.box.add(self.navigation)\n\n      self.show_all()\n\n\n   def plotSin(self, _unused_widget):\n      self.update(self.time, self.sin)\n\n\n   def plotCos(self, _unused_widget):\n      self.update(self.time, self.cos)\n\n\n   def update(self, x, y):\n      self.axis.lines[0].set_data(x, y)\n      self.autoScaleY()\n\n\n   def autoScaleY(self):\n      self.axis.relim()\n      self.axis.autoscale(axis='y')\n      self.canvas.draw()\n\n\nwin = PlotDemo()\nwin.connect(\"destroy\", Gtk.main_quit)\nwin.show_all()\nGtk.main()\n</code></pre><p><strong>重现问题的步骤：</strong></p><pre><code>1. Run script. Plotted SIN data should be visible.\n2. Click \"Zoom to rectangle\" button on toolbar\n3. Zoom in on the SIN plot\n4. Click \"Reset original view\" button on toolbar\n5. Click \"COS\" button at top\n6. Initial view should be correct, since I auto-scale the Y-Axis\n7. Zoom (optional), and then click \"Reset original view\" button again\n</code></pre><p><strong>问题：</strong>步骤7缩小到原始Y轴限制[-100100]</p><p><strong>所需行为：</strong>将视图重置为新数据集的适当限制[-1，1]</p></div>"}
{"href": "https://www.cnpython.com/qa/1473124", "title": "dockercompose with python代码返回错误13：权限被拒绝。'/.eggs'", "content": "<div class=\"show-content\"><p>我正试图让docker compose自动安装一些东西，以便与jenkins一起进行测试，在容器完成制作并访问这些命令之前，它会自动终止并吐出一堆文本。这是最后一点</p><pre><code>ipv6_address=None, links=[], link_local_ips=None)\ncompose.cli.verbose_proxy.proxy_callable: docker connect_container_to_network -&gt; None\ncompose.cli.main.call_docker: /usr/bin/docker start --attach --interactive 14adaad4b5a6ba50c2c2238e55218c73dbd32ee129f19517747b0156c8433613\n\nTraceback (most recent call last):\n  File \"setup.py\", line 337, in &lt;module&gt;\n    version=open('VERSION').readline().strip(),\n  File \"/usr/lib/python2.7/site-packages/setuptools/__init__.py\", line 161, in setup\n    _install_setup_requires(attrs)\n  File \"/usr/lib/python2.7/site-packages/setuptools/__init__.py\", line 156, in _install_setup_requires\n    dist.fetch_build_eggs(dist.setup_requires)\n  File \"/usr/lib/python2.7/site-packages/setuptools/dist.py\", line 721, in fetch_build_eggs\n    replace_conflicting=True,\n  File \"/usr/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 782, in resolve\n    replace_conflicting=replace_conflicting\n  File \"/usr/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 1065, in best_match\n    return self.obtain(req, installer)\n  File \"/usr/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 1077, in obtain\n    return installer(requirement)\n  File \"/usr/lib/python2.7/site-packages/setuptools/dist.py\", line 777, in fetch_build_egg\n    return fetch_build_egg(self, req)\n  File \"/usr/lib/python2.7/site-packages/setuptools/installer.py\", line 101, in fetch_build_egg\n    eggs_dir = os.path.realpath(dist.get_egg_cache_dir())\n  File \"/usr/lib/python2.7/site-packages/setuptools/dist.py\", line 762, in get_egg_cache_dir\n    os.mkdir(egg_cache_dir)\nOSError: [Errno 13] Permission denied: './.eggs'\ncompose.cli.verbose_proxy.proxy_callable: docker remove_container &lt;- ('14adaad4b5a6ba50c2c2238e55218c73dbd32ee129f19517747b0156c8433613', force=True, v=True)\n\ncompose.cli.verbose_proxy.proxy_callable: docker remove_container -&gt; None\ncompose.cli.main.exit_with_metrics: 1\n\n</code></pre><p>我一直在寻找一个原因，但我所看到的一切都不起作用。实际上，我已经使用<code>chmod 777 -R python2.7</code>将整个python.2.7目录的访问权限设置为完全访问\n我已经卸载并重新安装了python和docker，确保两个版本兼容，因为此时无法将项目更新为python3。我已经确保有一个docker用户和docker组，以及sudo权限。命令为<code>docker-compose run --rm rpm.build.{$PROJECT_NAME}</code>，其中<code>$PROJECT_NAME</code>解析为<code>Dockerfile.$PROJECT_NAME</code>。以前，它运行了99%的构建，但莫名其妙地失败了，从那以后一直给我带来新的问题。我无法在终端中使用docker compose，因为docker-compose.yml文件位于git repo上</p><p>编辑以添加dockerfile和docker-compose.yml的内容：</p><pre class=\"lang-yaml prettyprint-override\"><code>version: '2'\n\nservices:\n    _app:\n    build:\n        # args:\n        # REPO: ${REPO}\n        context: .\n\n    _build.rpm:\n        tty: false\n        build:\n            args:\n                GID: ${GID}\n                UID: ${UID}\n            context: .\n            dockerfile: Dockerfile.el6\n        command: python2 setup.py bdist_rpm2\n        volumes:\n            - .:/usr/src/front\n        working_dir: /usr/src/proj\n\n    app:\n        extends: _app\n        command: project runserver 0.0.0.0:8080\n        environment:\n            PROJECT_DATABASE_HOST: database\n            PROJECT_DATABASE_NAME: postgres\n            PROJECT_DATABASE_USER: postgres\n            PROJECT_DEBUG: 1\n            PROJECT_TEST: 0\n        expose:\n            - \"8080\"\n        links:\n            - cron\n            - database\n        ports:\n            - \"8080:8080\"\n        volumes:\n            - ./debug:/usr/src/debug\n            - ./project:/opt/project/lib/project\n\n    fastx:\n        build:\n            context: .\n            dockerfile: Dockerfile.fastx\n        working_dir: /tmp\n        command: tail -f /var/log/yum.log\n\n    build.rpm.centos6:\n        extends: build.rpm.el6\n\n    build.rpm.centos7:\n        extends: build.rpm.el7.centos\n\n    build.rpm.el6:\n        build:\n            dockerfile: Dockerfile.el6\n        extends: _build.rpm\n\n    build.rpm.el7.centos:\n        build:\n            dockerfile: Dockerfile.el7.centos\n        extends: _build.rpm\n\n    build.rpm.sles12:\n        extends: build.rpm.suse\n\n    build.rpm.suse:\n        build:\n            dockerfile: Dockerfile.suse\n        extends: _build.rpm\n\n    fastx.rebuild.el6:\n        tty: false\n        build:\n            context: buildutils/fastx-branding/\n            dockerfile: Dockerfile.el6\n        command: /opt/fastx-branding/rebuild-fastx.sh -r CentOS5 -n StarNetFastX2-2.2-77.3.x86_64.rpm\n        working_dir: /opt/fastx-branding\n        volumes:\n            - ./buildutils/fastx-branding/:/opt/fastx-branding\n\n    fastx.rebuild.centos6:\n        extends: fastx.rebuild.el6\n\n    fastx.rebuild.el7:\n        tty: false\n        build:\n            context: buildutils/fastx-branding/\n            dockerfile: Dockerfile.el7\n        command: /opt/fastx-branding/rebuild-fastx.sh -r CentOS5 -n StarNetFastX2-2.2-77.3.x86_64.rpm\n        working_dir: /opt/fastx-branding\n        volumes:\n            - ./buildutils/fastx-branding/:/opt/fastx-branding\n\n    fastx.rebuild.centos7:\n        extends: fastx.rebuild.el7\n\n    fastx.rebuild.suse:\n        tty: false\n        build:\n            context: buildutils/fastx-branding/\n            dockerfile: Dockerfile.suse\n        command: /opt/fastx-branding/rebuild-fastx.sh -r CentOS5 -n StarNetFastX2-2.2-77.3.x86_64.rpm\n        working_dir: /opt/fastx-branding\n        volumes:\n        - ./buildutils/fastx-branding/:/opt/fastx-branding\n\n    fastx.rebuild.sles12:\n        extends: fastx.rebuild.suse\n\n    cron:\n        extends: _app\n        command: crond -n\n\n    database:\n        image: postgres\n        ports:\n            - \"5432:5432\"\n\nnetworks:\n    default:\n        external:\n            name: reporting\n</code></pre><p>这是docker-compose.yml，现在是相关的Dockerfile.centos7</p><pre><code>FROM centos:7\n\n##MAINTAINER goes here., I left it out because its not technically me.\n\nARG GID=500\n\nARG UID=500\n\nRUN groupadd -g 500 user &amp;&amp; useradd -g user -u 500 user\nRUN chown user:user -R /app/\nRUN yum -y install epel-release &amp;&amp; \\\nyum -y install which libffi-devel gcc-c++ patch readline readline-devel zlib zlib-devel sudo \\\nlibyaml-devel libffi-devel openssl-devel make \\\nbzip2 autoconf automake libtool bison iconv-devel sqlite-devel &amp;&amp; \\\ngpg --keyserver hkp://pool.sks-keyservers.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB\n\nRUN echo 'export rvm_prefix=\"$HOME\"' &gt; /root/.rvmrc &amp;&amp; \\\necho 'export rvm_path=\"$HOME/.rvm\"' &gt;&gt; /root/.rvmrc &amp;&amp; \\\ncurl -L get.rvm.io |rvm_path=/opt/rvm bash -s stable &amp;&amp; \\\nsource /root/.rvm/scripts/rvm &amp;&amp; \\\n\nRUN /bin/bash -l -c 'rvm get latest' &amp;&amp; \\\n/bin/bash -l -c 'rvm reload' &amp;&amp; \\\n/bin/bash -l -c 'rvm autolibs' &amp;&amp; \\\n/bin/bash -l -c 'rvm requirements' &amp;&amp; \\\n/bin/bash -l -c 'rvm install ruby' &amp;&amp; \\\nruby -v &amp;&amp; \\\n/bin/bash -l -c 'rvm --default use 3.0.0' &amp;&amp; \\\necho 'attempt at disabling autolibs' &amp;&amp; \\\nruby -v &amp;&amp; \\\n/bin/bash -l -c 'yum -y install rpm-build rubygems' &amp;&amp; \\\n/bin/bash -l -c 'gem install sass'\n\nRUN yum -y install \\\ngcc \\\ngit \\\nhttpd \\\nlibffi-devel \\\nlibxslt-devel \\\nnpm \\\npostgresql-devel \\\npython27-devel &amp;&amp; \\\ncurl https://bootstrap.pypa.io/pip/2.7/get-pip.py | python &amp;&amp; \\\nnpm install -g \\\nbower \\\ngulp \\\nuglifyjs \\\nyuglify &amp;&amp; \\\npip install https://codeload.github.com/typeundefined/pyobfuscate/tar.gz/pyobfuscate-0-4\n\n# XXX: fix bug of pip 8.0.2 and pyobfuscate's setup.py\nRUN ln -s /usr/lib/python/site-packages/usr/bin/pyobfuscate /usr/bin/pyobfuscate\n\nRUN ln -s rlm.a /usr/lib64/librlm.a &amp;&amp; \\\nmkdir /usr/include/rlm\n\nCOPY lib/rlm/license.h /usr/include/rlm/\n\nCOPY lib/rlm/rlm.a /usr/lib64/\n\nUSER user\n</code></pre><p>最后，如果与linw相关，则不起作用的是：</p><pre class=\"lang-sh prettyprint-override\"><code>COMPOSE_HTTP_TIMEOUT=120 docker-compose --verbose run --rm build.rpm.${JENKINS_SUFFIX}\n</code></pre><p>我真的希望有足够的信息来帮助你帮助我</p></div>"}
{"href": "https://www.cnpython.com/qa/1473125", "title": "使用Python数据类输出原始JSON", "content": "<div class=\"show-content\"><p>我使用Python数据类来帮助将我的SQL Alchemy数据模型输出为JSON<a href=\"https://stackoverflow.com/a/57732785\">as outlined here</a>。问题是我的类属性之一是存储在数据库中的序列化非结构化JSON数据字符串，我希望能够将其作为原始JSON输出。问题是，我似乎找不到允许此操作的正确数据类型。我尝试过dict、json和object，但所有这些仍然会导致json字段中填充一个分隔字符串，如标记为“current output”的区域中所示。有没有一种方法可以在数据类中实现这一点</p><pre><code>class Event(db.Model):\n    id: int\n    data: ???\n\n    __tablename__ = \"data\"\n    id = db.Column('id', db.Integer, primary_key=True, autoincrement=True)\n    data = db.Column('data', db.Text, nullable=True)\n</code></pre><p>电流输出：</p><pre><code>   {\n     \"id\": 1,\n     \"data\": \"{\\\"data\\\": \\\"this is data\\\"}\"\n   }\n</code></pre><p>预期产出：</p><pre><code>   {\n     \"id\": 1,\n     \"data\": {\"data\": \"this is data\"}\n   }\n</code></pre></div>"}
{"href": "https://www.cnpython.com/qa/1473126", "title": "pip安装成功，但命令不起作用", "content": "<div class=\"show-content\"><p>我正在尝试pip安装CLI，它已成功安装</p><pre><code>pip3 install databricks-cli\nWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\nPlease see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: databricks-cli in ./Library/Python/3.8/lib/python/site-packages (0.14.3)\nRequirement already satisfied: click&gt;=6.7 in ./Library/Python/3.8/lib/python/site-packages (from databricks-cli) (7.1.2)\nRequirement already satisfied: requests&gt;=2.17.3 in ./Library/Python/3.8/lib/python/site-packages (from databricks-cli) (2.25.1)\nRequirement already satisfied: six&gt;=1.10.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from databricks-cli) (1.15.0)\nRequirement already satisfied: tabulate&gt;=0.7.7 in ./Library/Python/3.8/lib/python/site-packages (from databricks-cli) (0.8.9)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in ./Library/Python/3.8/lib/python/site-packages (from requests&gt;=2.17.3-&gt;databricks-cli) (1.26.4)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in ./Library/Python/3.8/lib/python/site-packages (from requests&gt;=2.17.3-&gt;databricks-cli) (2.10)\nRequirement already satisfied: chardet&lt;5,&gt;=3.0.2 in ./Library/Python/3.8/lib/python/site-packages (from requests&gt;=2.17.3-&gt;databricks-cli) (4.0.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in ./Library/Python/3.8/lib/python/site-packages (from requests&gt;=2.17.3-&gt;databricks-cli) (2020.12.5)\n</code></pre><p>但是当我继续尝试使用一个命令时，我得到了一个错误，它找不到该命令</p><pre><code> ~ databricks --version\nzsh: command not found: databricks\n</code></pre></div>"}
{"href": "https://www.cnpython.com/qa/1473127", "title": "使用pyhocon更新HOCON文件", "content": "<div class=\"show-content\"><p>我有一个HOCON文件模板，带有一些属性，比如配置。此文件针对不同的“名称”（由bu用户输入）进行更新并上载。我试图通过拉模板、更新必要的值并上传更新后的文件来构建hocon文件</p><pre><code>deployment {\n      proxy {\n        // Name has to be replaced with the name of the project\n        cluster.NAME {\n          property1 = [a_list]\n          property2.host = \"hostname\"\n        }\n      }\n    }\n</code></pre><p>我可以使用pyhocon更新值：</p><pre><code>from pyhocon import ConfigFactory\n\n\nconf = ConfigFactory.parse_string(hocon_file_template)\nhost = \"something-TEST.trial.com\"\nconf.put('deployment.proxy.cluster.NAME.property2.host', host)\nnew = HOCONConverter.convert(conf, \"hocon\")\n</code></pre><p>但是，我需要用用户输入替换“cluster.NAME”中的“NAME”，比如“TEST”。我尝试使用put更改名称，但这会附加到集群树，而不是将名称更新为“TEST”</p><pre><code>host_key = 'deployment.proxy.cluster.' \".{}.property2.host\"\nconf.put(host_key.format(user_input), host)\n</code></pre><p>如何将名称更新为输入值（在本例中为“TEST”）</p></div>"}
{"href": "https://www.cnpython.com/qa/1473128", "title": "用地理编码Python对循环进行矢量化", "content": "<div class=\"show-content\"><p>我有一个很长的地址列表，我需要用坐标对它们进行地理编码，我正在用Python中的geopy进行这项工作。我写了一个循环，以便为每次观测找到相应的坐标。在这个循环中，我还考虑了这样一个事实，即有时存在连接超时问题（因此它会重新尝试地理编码），有时它无法找到坐标（返回none）。问题是速度非常慢，我在半小时内成功地对1000个OB进行了地理编码，所以我想知道是否有办法加快速度，比如矢量化</p><p>我可以减少重新尝试的等待时间，但更多尝试将失败</p><p>这是一个示例代码：</p><pre><code>import pandas as pd\nfrom geopy.geocoders import Nominatim\nimport numpy as np\nimport time\n\ngeolocator = Nominatim(user_agent = 'local_agent')\n\ndef geocode_address(address):\n    g = geolocator.geocode(address)\n    return g\n\ndef try_address(address, attempts_remaining, wait_time):\n    g = geocode_address(address)\n    if g is None:\n        time.sleep(wait_time)\n        if attempts_remaining &gt; 0:\n            try_address(address, attempts_remaining-1, wait_time+wait_time)\n    return g\n\nstart_index = 0\n# How often the program prints the status of the running program\nstatus_rate = 100\n# How many times the program tries to geocode an address before it gives up\nattempts_to_geocode = 2\n# Time it delays each time it does not find an address\nwait_time = 3\n\n# Variables used in the main for loop \nresults = []\nfailed = 0\ntotal_failed = 0\nprogress = len(df) - start_index\n\nfor i, address in enumerate(df[\"address\"]):\n    # Print the status of how many addresses have be processed so far and how many of the failed.\n    if (start_index + i) % status_rate == 0:\n        total_failed += failed\n        print(\"Completed {} of {}. Failed {} for this section and {} in total.\"\n              .format(i + start_index, progress, failed, total_failed))\n        failed = 0\n    # Try geocoding the addresses\n    try:\n        g = try_address(address, attempts_to_geocode, wait_time)\n        if g is None:\n            results.append([address, \"\", \"\", \"None\"])\n            print(\"Gave up on address: \" + address)\n            failed += 1\n        else:\n            results.append([address, g.latitude, g.longitude, \"ArcGIS\"])\n    # If we failed with an error like a timeout we will try the address again after we wait 5 secs\n    except Exception as e:\n        print(\"Failed with error {} on address {}. Will try again.\".format(e, address))\n        try:\n            time.sleep(5)\n            g = geocode_address(address)\n            if g is None:\n                print(\"Did not find it.\")\n                results.append([address, \"\", \"\", \"None\"])\n                failed += 1\n            else:\n                print(\"Successfully found it.\")\n                results.append([address, g.latitude, g.longitude, \"ArcGIS\"])\n        except Exception as e:\n            print(\"Failed with error {} on address {} again.\".format(e, address))\n            failed += 1\n            results.append([address, \"\", \"\", \"Error\"])\n</code></pre></div>"}
{"href": "https://www.cnpython.com/qa/1473129", "title": "比较数据帧中长度不同的两个列表", "content": "<div class=\"show-content\"><p>我有两个带有列表的数据帧，我需要对它们进行比较，以便知道一行是否与另一行相同，然后将其添加</p><p>这两个数据集是：</p><p><strong>数据集1</strong></p><pre><code>sex     age     zip_code    \n0   F   46  08204   [6, 40, 9, 44, 30]\n1   F   40  08205   [33, 41, 48, 50, 30]\n2   F   46  08206   [32, 33, 4, 37, 43, 21]\n3   F   60  08304   [37, 39, 7, 42, 11, 49]\n4   M   24  08507   [32, 15, 23, 25, 29]\n5   M   42  08917   [9, 42, 41, 13, 45, 23]\n6   F   50  08921   [10, 50, 29, 52]\n7   F   37  10627   [41, 3, 29, 39]\n</code></pre><p><strong>数据集2</strong></p><pre><code>user_id           weeks                      blood_levels\n0   1101    [15, 23, 25, 29, 32]    [126, 127, 120, 111, 107]\n1   1122    [33, 41, 48, 50, 30]    [70, 72, 69, 68, 74, 76, 72]\n2   2112    [4, 10, 11, 16, 17, 18, 24, 31, 33, 36]     [117, 96, 114, 99, 119, 100, 93, 87, 80, 74]\n3   2200    [3, 5, 7, 14, 21, 22, 27, 28, 34]   [152, 126, 165, 169, 167, 169, 140, 154, 157]\n</code></pre><p>我试图比较的是，dataset2中“weeks”列的列表位于DataSet1中“weeks”列的列表中（全部）。如果这是真的，则应将数据集2的“用户id”和“血液级别”列添加到数据集1。因此，<strong>预期输出：</strong></p><pre><code>sex     age     zip_code    weeks          user_id      blood_levels\n0   F   46  08204   [6, 40, 9, 44, 30]       1101         [126, 127, 120, 111, 107]\n1   F   40  08205   [33, 41, 48, 50, 30]     1122         [70, 72, 69, 68, 74, 76, 72]\n2   F   46  08206   [32, 33, 4, 37, 43, 21]\n3   F   60  08304   [37, 39, 7, 42, 11, 49]\n4   M   24  08507   [32, 15, 23, 25, 29]\n5   M   42  08917   [9, 42, 41, 13, 45, 23]\n6   F   50  08921   [10, 50, 29, 52]\n7   F   37  10627   [41, 3, 29, 39]\n</code></pre><p>请注意，并非数据集2的所有周都在数据集1中。\n到目前为止，我尝试的是：</p><pre><code>df_1['intersection'] = [list(set(a).intersection(set(b))) for a, b in zip(df_one.weeks, df_two.weeks)]\n</code></pre><p>然而，它给出了不同长度的误差</p><p><strong>添加了样本数据：</strong></p><pre><code>df1 = {'sex': {0: 'F', 1: 'F', 2: 'F', 3: 'F', 4: 'M'},\n 'age': {0: 46, 1: 40, 2: 46, 3: 60, 4: 24},\n 'zip_code': {0: '08204', 1: '08205', 2: '08206', 3: '08304', 4: '08507'},\n 'weeks': {0: [6, 40, 9, 44, 30],\n  1: [33, 41, 48, 50, 30],\n  2: [1, 2, 6, 10, 13, 19, 25, 26, 27, 29],\n  3: [4, 10, 11, 16, 17, 18, 24, 31, 33, 36],\n  4: [32, 15, 23, 25, 29]}}\n\ndf2 = {'user_id': {0: 1101, 1: 1122, 2: 2112, 3: 2200, 4: 3010},\n 'weeks': {0: [15, 23, 25, 29, 32],\n  1: [8, 9, 12, 20, 25, 30, 35],\n  2: [4, 10, 11, 16, 17, 18, 24, 31, 33, 36],\n  3: [3, 5, 7, 14, 21, 22, 27, 28, 34],\n  4: [1, 2, 6, 10, 13, 19, 25, 26, 27, 29]},\n 'blood_levels': {0: [126, 127, 120, 111, 107],\n  1: [70, 72, 69, 68, 74, 76, 72],\n  2: [117, 96, 114, 99, 119, 100, 93, 87, 80, 74],\n  3: [152, 126, 165, 169, 167, 169, 140, 154, 157],\n  4: [99, 82, 74, 69, 91, 96, 96, 68, 78, 89]}}\n</code></pre><p>你能帮我做到这一点吗</p><p>多谢各位</p></div>"}
{"href": "https://www.cnpython.com/qa/1473130", "title": "pygame.event和pygame.fastevent之间有什么区别？", "content": "<div class=\"show-content\"><p>根据<code>pygame.event</code>文档，此函数从队列中获取所有事件</p><blockquote><p>get events from the queue<br><code>get(eventtype=None)</code> -&gt; Eventlist<br><code>get(eventtype=None, pump=True)</code> -&gt; Eventlist<br>\nThis will get all the messages and remove them from the queue.</p></blockquote><p>根据<code>pygame.fastevent</code>文档，此函数从队列中获取所有事件</p><blockquote><p>get all events from the queue<br><code>get()</code> -&gt; list of Events<br>\nThis will get all the messages and remove them from the queue.</p></blockquote><h3>那么有什么区别呢？</h3><p>我认为区别在于多线程：<code>event</code><em>应该从主线程调用，^{</em>和<code>fastevent</code>用于<em>“多线程环境”</em>——但我看不到任何区别（例如在<a href=\"https://stackoverflow.com/questions/66625655/getting-pygame-events-from-another-thread\">this</a>情况下）</p></div>"}
{"href": "https://www.cnpython.com/qa/1473131", "title": "将一个组划分为n，并在python中为每个组添加块号", "content": "<div class=\"show-content\"><p>我有下表：</p><div class=\"s-table-container\">\n^{tb1}$\n</div><p>我想将这个表分成“n”（这里是n=4）组，并添加另一个组名为的列。输出应如下所示：</p><div class=\"s-table-container\">\n^{tb2}$\n</div><p>我这样做是为了什么</p><pre><code>TGn = 4\nidx = set(df.index // TGn)\n\ntreatment_groups = [i for i in range(1, n+1)]\ndf['columnC'] = (df.index // TGn).map(dict(zip(idx, treatment_groups)))\n</code></pre><p>这并没有正确地划分团队，也不知道我错在哪里。我如何纠正它</p></div>"}
{"href": "https://www.cnpython.com/qa/1473132", "title": "如何使用Scrapy的爬行蜘蛛从一个页面和每一个点击子页面中刮取所有网站文本", "content": "<div class=\"show-content\"><p>我想刮了大量的网站的文本显示给网站用户。我正在使用Scrapy来执行此任务。到目前为止，我已经使用了base spider并编写了</p><ol><li>检索网站的整个HTML代码</li><li>使用BeautifulSoup的get_text（）解析文本的HTML代码</li><li>保存包含文本信息和HTML代码的单独文件</li></ol><p>这段代码做了我想做的一切，只是它只会刮去主页面。我的目标是修改代码，使其能够抓取并保存主页的文本，找到指向所有内部子网站的链接（只需单击一次），并抓取这些网站的文本。下面是一个简化的基本spider的最小工作示例</p><pre><code>import scrapy\nfrom bs4 import BeautifulSoup\n\nurl_list = [\n        'https://www.apple.com',\n        'https://www.macrumors.com',\n        ]\n\nclass TXTSpider(scrapy.Spider):\n    name = 'mwe_spider'\n    start_urls = url_list\n\n\n    def parse(self, response):\n\n        current_url = response.request.url\n        filename = current_url.split(\".\")[-2]\n\n        html = response.body\n\n        soup = BeautifulSoup(html, 'lxml')\n\n        text = soup.get_text()\n\n        with open(f'{filename}.txt', 'w') as fp:\n            fp.write(text)\n\n        with open(f'{filename}.html', 'wb') as fp:\n            fp.write(response.body)\n\n        self.log(f'Saved file {filename}')\n</code></pre><p>我试着修改这个代码来实现我的目标。具体来说，我发现Scrapy的爬行蜘蛛类提供了所需的功能。根据我所阅读的内容，我对MWE做了以下更改：</p><ol><li>将蜘蛛类从Scrapy的基础蜘蛛更改为爬行蜘蛛</li></ol><p>通过导入和实现爬行爬行器类（<code>class TXTSpider(CrawlSpider):</code>），以及将解析函数重命名为<code>def parse_item(self, response):</code>，以避免对爬行爬行器（<a href=\"https://stackoverflow.com/questions/44620722/scrapy-crawlspider-crawls-nothing\">Link</a>）的干扰。最后，我将spider限制在我感兴趣的网站上，以避免外部网站：<code>allowed_domains = ['apple.com', 'macrumors.com']</code>）</p><ol start=\"“2”\"><li>添加了一条规则，指示爬行器应遵循所有网站链接</li></ol><p>据我所知，以下规则实现了这一目标，同时避免了与图像等的链接：<code>rules = (Rule(LinkExtractor(), callback='parse_page', follow=True),)</code></p><ol start=\"“3”\"><li><p>保留了我原来的parse函数（重命名为parse_item），因为我仍然想保存主页上的文本和HTML信息</p></li><li><p>添加了一个额外的解析函数（parse_page），该函数应该能够从链接的内部子网站中获取文本信息（无需再次解析链接，因为我只对距离主页单击一次的站点感兴趣）。parse_页面类似于第一个parse函数，但将文本信息附加到主页的文本文件中，以生成站点文本的一个合并文件。编写新HTML文件的代码保持不变，以便为子页面接收单独的HTML文件</p></li></ol><p>生成的代码粘贴在下面：</p><pre><code>import scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urlparse\nimport tldextract\n\n\nurl_list = [\n        'https://www.apple.com',\n        'https://www.macrumors.com',\n        ]\n\nclass TXTSpider(CrawlSpider):\n    name = 'mwe_crawlspider'\n    allowed_domains = ['apple.com',\n                       'macrumors.com'\n                       ]\n\n    start_urls = url_list\n\n    rules = (\n        Rule(LinkExtractor(), callback='parse_page', follow=True),\n    )\n\n\n    def parse_page(self, response):\n\n        html = response.body\n        soup = BeautifulSoup(html, 'lxml')\n        text = soup.get_text()\n\n        domain = tldextract.extract(response.request.url)[1]\n        path = urlparse(response.request.url)[2].replace(\"/\", \"\")\n\n        with open(f'{domain}.txt', 'a') as fp:\n            fp.write(text)\n\n        with open(f'{domain} {path}.html', 'wb') as fp:\n            fp.write(response.body)\n</code></pre><p>非常感谢你的帮助</p><p><strong>编辑1:</strong>代码已更新，以在第二个解析函数中重新定义<code>filename</code></p><p><strong>编辑2:</strong>将<code>DEPTH_LIMIT=1</code>添加到settings.py文件会将爬行爬行器限制在距提供的url单击一次的页面上</p><p><strong>编辑3:</strong>更新了上面的代码，该代码现在执行所有必需的任务。使用<code>tldextract</code>提取网站域可确保仅将文本信息附加到一个文本文件中，即使同一网站的子页面使用不同的URL格式（例如<a href=\"http://www.apple.com\" rel=\"nofollow noreferrer\">www.apple.com</a>和tv.apple.com）<code>urlparse</code>用于查找路径信息，以便为每个子页面保存单个HTML文件</p></div>"}
{"href": "https://www.cnpython.com/qa/1473153", "title": "TypeError:nltk中应为字符串或byteslike对象", "content": "<div class=\"show-content\"><p>所以，我对nlp还比较陌生，我正在努力提高自己。目前正在处理电影数据集“imdb 50k电影评论数据集”。在我的第一步中，我使用以下代码将段落分解为句子：</p><pre><code># Defining an empty array for saving tokens\n  tokenized_to_sentences = []\n\n  for index, row in x_train.iterrows():\n      tokenize_review = x_train[\"review\"][index]\n      result = nltk.tokenize.sent_tokenize(tokenize_review)\n      tokenized_to_sentences.append(result)\n</code></pre><p>上面的代码工作得很好，但当我尝试将其进一步分解为“单词标记”时，它返回以下错误：</p><pre><code>TypeError: expected string or bytes-like object\n</code></pre><p>尝试了不同的代码，例如：</p><pre><code>tokenized_to_words = []\n\nfor i in tokenized_to_sentences:\ntokenization = \nnltk.word_tokenize(tokenized_to_sentences)\ntokenized_to_words.append(tokenization)`\n</code></pre><p>但问题依然存在。我的目标是在每个句子中重复每个句子，并将其标记为单词</p></div>"}
{"href": "https://www.cnpython.com/qa/1473134", "title": "如何在python中使用plotly/dash获取图像选定区域的坐标？", "content": "<div class=\"show-content\"><p>在plotly中选择图像区域时，如何获取坐标？<br>\n例如，我希望获得圆选择的（X，Y，半径）（如图所示）<br><img src=\"https://i.stack.imgur.com/qK0qz.png\" alt=\"cat drawcircle and drawrect\"></p><p>如果不可能详细说明，那么备选方案是什么</p></div>"}
{"href": "https://www.cnpython.com/qa/1473135", "title": "使用Pytorch预测棋盘位置", "content": "<div class=\"show-content\"><p>我想用Pytork/keras预测当前国际象棋棋盘的一个趋势。（现在我们不必担心输入。）</p><p>我怎么知道的？\n棋盘上每个位置有8x8个位置（64个），可以是黑色或白色棋子（12个），也可以没有棋子（1个）。我计划在国际象棋棋盘上使用此表示法（欢迎其他建议！）：\n<a href=\"https://en.wikipedia.org/wiki/Board_representation_(computer_chess)#Square_list\" rel=\"nofollow noreferrer\">https://en.wikipedia.org/wiki/Board_representation_(computer_chess)#Square_list</a><br>\n例如：</p><pre><code> 2 3 4 5 6 4 3 2\n 1 1 1 1 1 1 1 1\n 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0\n-1-1-1-1-1-1-1-1\n-2-3-4-5-6-4-3-2`\n</code></pre><p>据我所知，这样的事情是不可能预测的。因为我的最后一层必须预测的类数是448（64x7），我觉得NN无法做到这一点。此外，还有一个问题，即softmax无法工作（imo）。损失函数也可能成为一个问题</p><p>有人知道怎么做吗？或者可以为我指出正确的方向，因为多类分类并不是这个任务的正确术语。我在考虑创建6个网络，为每件作品创建一个分类。因此，8x8阵列的外观如下（适用于rooks）：</p><pre><code> 10000001\n 00000000\n 00000000\n 00000000\n 00000000\n-1000000-1\n</code></pre><p>但问题仍然很相似。\n我认为创建64个NN，每个NN负责一个位置，这将稍微简化问题。但这将是一个痛苦的训练。\n期待您的建议</p></div>"}
{"href": "https://www.cnpython.com/qa/1473136", "title": "如何检查一个数字是否可以表示为x乘以y的幂？", "content": "<div class=\"show-content\"><p>我需要x^y==整数（输入）\n我想不出怎么做\n<em><strong>我无法使用数学导入\n试图做这样的事情：</strong></em></p><pre><code>a = int(input(\"Please Enter any Positive Integer:\"))\n power = 1\n i = 1\n\n while(i &lt;= a):\n     power = power * a\n    i = i + 1\n</code></pre><p>比如说\n输入是8\n我需要这个程序来找到x^y==8\n在这种情况下，输出需要：\nx=2\ny=3\n希望它清楚\n提前谢谢</p></div>"}
{"href": "https://www.cnpython.com/qa/1473137", "title": "mypy：将泛型类型的变量与任何类型的变量进行比较", "content": "<div class=\"show-content\"><p>如何将泛型类型的属性与定义良好的类型的变量进行比较</p><p>我有一个名为<code>ResultWrapper</code>的类，它具有泛型类型的属性</p><pre><code>from typing import TypeVar, Generic, Optional\nfrom attr import dataclass\n\nT = TypeVar(\"T\")\nU = TypeVar(\"U\")\n\n@dataclass\nclass ResultWrapper(Generic[U, T]):\n    eval: Optional[U] = None\n    student: Optional[T] = None\n\ndef get_wrappers(evals: List[U], students: List[T]) -&gt; List[ResultWrapper]:\n    # Returns a list of wrappers.\n    return [ResultWrapper(eval, student) for eval, student in zip(evals, students)]\n</code></pre><p>在测试中，我将包装器对象与预期结果进行比较：</p><pre><code>from typing import List, Any\n\ndef compare_wrappers(wrappers: List[ResultWrapper], evals: List[Any], students: List[Any]):\n    for wrapper, eval, student in zip(wrappers, evals, students):\n        assert wrapper.eval == eval\n        assert wrapper.student == student\n\nactual = get_wrappers([1, 2], [\"Bob\", \"Frank\"])\ncompare_wrappers(actual, [1, 2], [\"Bob\", \"Frank\"])\n</code></pre><p>这会引发错误，因为<code>eval</code>和<code>student</code>属性是泛型类型，无法与我传递的整数字符串列表进行比较</p><pre><code>    assert wrapper.eval == eval\nE           AssertionError: assert ~U == 1\nE            +  where ~U = ResultWrapper(eval=~U, student=~T).\n</code></pre><p>如何将泛型类型的属性与定义良好的类型的变量进行比较</p><p>我不想将<code>eval</code>变量强制转换为<code>compare_wrappers()</code>中的int，因为<code>eval</code>可以有任何类型。有没有办法动态获取<code>eval</code>的类型并将其强制转换为该类型，而不是硬编码该类型</p></div>"}
{"href": "https://www.cnpython.com/qa/1473133", "title": "当我向CNN添加参数class_weight时，内核死亡", "content": "<div class=\"show-content\"><p>基本上，只要在内核中插入“class_weight”，内核就会死亡。我已经运行了完全相同的模型，但没有类_重量，没有问题。我正在使用MacBookPro M1芯片16gb RAM、python 3.8和tensorflow 2.4上的jupyter笔记本（6.2.0）。\n多谢各位</p></div>"}
{"href": "https://www.cnpython.com/qa/1473154", "title": "ValueError:x和y必须具有相同的第一维度，但具有形状（32，）和（30，）", "content": "<div class=\"show-content\"><p>我正在研究一个股票预测模型，我得到了这个错误，尽管它们有相同的值</p><p>这是我训练模型并使用线性回归的代码</p><pre><code>    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2)\n    clf =  LinearRegression()\n    clf.fit(x_train, y_train)\n    scorepredict = clf.score(x_test,y_test)\n    predictedprice = clf.predict(x_prediction)\n</code></pre><p>这里是我得到错误的地方</p><pre><code>    dates = pd.date_range(start = \"2018-03-28\", end  = \"2018-04-28\")\n    plt.plot(dates,color =\"y\")\n    df[\"Adj. Close\"].plot(color = \"g\")\n    plt.xlim(xmin=datetime.date(2017,1,1))\n</code></pre><p>这里是我设置x和y阵列的地方</p><pre><code>predictiondays = 30\ndf[\"Prediction\"] = df[[\"Adj. Close\"]].shift(-predictiondays)\nx = np.array(df.drop([\"Prediction\"],1))\nx = preprocessing.scale(x)\nx_prediction = x[-predictiondays:]\nx = x[:-predictiondays]\n\ny = np.array(df[\"Prediction\"])\ny = y[:-predictiondays]\n</code></pre><p>编辑：</p><p>现在，在clf.fit中获取以下错误：</p><p>ValueError:应为2D数组，而应为1D数组：\n数组=[-1.23874374 1.49125839 0.80930081…0.32190385 1.01987874\n1.06322504]. </p><p>我什么都没碰，卢尔</p></div>"}
{"href": "https://www.cnpython.com/qa/1473155", "title": "Django中的嵌套管理命令", "content": "<div class=\"show-content\"><p>在Django中是否有创建嵌套管理命令的方法，类似于<code>docker</code>和<code>kubectl</code>的方法？例如，假设我需要具有以下结构：</p><pre><code>|--&gt;manage.py\n    |--&gt;restaurant\n        |--&gt;list\n        |--&gt;get\n    |--&gt;employee\n        |--&gt;list\n        |--&gt;get\n        |--&gt;delete\n</code></pre><p>以下命令都应该是可能的：</p><pre><code>./manage.py -h\n./manage.py restaurant -h\n./manage.py restaurant list\n./manage.py employee list\n./manage.py restaurant get \"\"\n./manage.py employee delete tiffany\n</code></pre><p>argparse<a href=\"https://docs.python.org/3/library/argparse.html#argparse.ArgumentParser.add_subparsers\" rel=\"nofollow noreferrer\">subparser</a>看起来很有希望，但我觉得应该有一种更简单的方法在<code>app/management/commands</code>或类似的地方使用python模块</p></div>"}
{"href": "https://www.cnpython.com/qa/1473157", "title": "在Python AutoScraper库中找不到AutoScraper模块", "content": "<div class=\"show-content\"><p>我想试试python的autoscraper库\n<a href=\"https://github.com/alirezamika/autoscraper\" rel=\"nofollow noreferrer\">https://github.com/alirezamika/autoscraper</a>\n我使用vir env Python 3.8在Pycharm中打开了新项目，并安装了autoscraper库。同时，添加了更多的包（如bs4等）。当我尝试导入AutoScraper对象时</p><pre class=\"lang-py prettyprint-override\"><code>from autoscraper import AutoScraper\n</code></pre><p>PyCharm找不到该模块。\n当我在<code>./lib/python3.8/site-packages/autoscraper/auto_scraper.py</code>中打开模块脚本时，它显示问题出在从包utils.py中的第二个文件导入时</p><p>我对我的步骤做了截图。你能告诉我我做错了什么，以及如何使包装正常工作吗</p><p>创建病毒环境：</p><p><img src=\"https://i.stack.imgur.com/2qmij.png\" alt=\"Creating the vir env\"></p><p>正在导入autoscraper库：</p><p><img src=\"https://i.stack.imgur.com/9jJjE.png\" alt=\"Importing autoscraper library\"></p><p>导入-未找到模块自动清除程序</p><p><img src=\"https://i.stack.imgur.com/aAGCz.png\" alt=\"Import - module AutoScraper not found\"></p><p>auto_scraper.py文件中出错\n<img src=\"https://i.stack.imgur.com/t2J0l.png\" alt=\"error in auto_scraper.py file\"></p><p>utils.py文件\n<img src=\"https://i.stack.imgur.com/4Nmid.png\" alt=\"utils.py file\"></p></div>"}
{"href": "https://www.cnpython.com/qa/1473156", "title": "用python编写表达式：将字符串乘以int", "content": "<div class=\"show-content\"><p>我有两个数据帧：一个有字符串，另一个有int。我试图将这两个数据帧相乘以生成一个代数表达式：</p><pre><code>df1=pd.DataFrame([1,2,3])\ndf2=pd.DataFrame([\"x\",\"y\",\"z\"])\n</code></pre><p><strong>预期输出：</strong><a href=\"https://i.stack.imgur.com/feAlj.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/feAlj.png\" alt=\"enter image description here\"></a></p><p>如何使用两种不同的数据类型生成表达式</p></div>"}
{"href": "https://www.cnpython.com/qa/1473161", "title": "如何使用函数打破for循环？", "content": "<div class=\"show-content\"><p>我正在使用for循环对元素列表运行函数中定义的if操作。第一个动作中有一个辅助动作。我希望在操作第一次成功后停止for循环。下面是要演示的示例代码：</p><pre><code>my_list = [99, 101, 200, 5, 10, 20, 40]\n\ndef action(x):\n    if x &gt;= 100:\n        print('It is finished')\n        over_100 = True\n        return over_100\n\ndef action2(x):\n    x += 1\n    action(x)\n    \nover_100 = False\nfor number in my_list:\n    action2(number)\n    if over_100:\n        break\n</code></pre><p>我希望for循环在&amp;gt=100例如，它应该将1添加到99（列表的第一个元素），然后停止所有操作。相反，它打印“ItIsFinished”3倍，因为它在整个列表中循环</p></div>"}
{"href": "https://www.cnpython.com/qa/1473158", "title": "卷积生成对抗网络的鉴别器的输出是如何工作的，它能有一个完全连接的层吗？", "content": "<div class=\"show-content\"><p>我正在构建一个DCGAN，输出的形状有问题，当我尝试计算BCELoss时，它与标签的形状不匹配</p><p>要生成鉴别器输出，我必须一直使用卷积，还是可以在某个点添加一个线性层以匹配我想要的形状</p><p>我的意思是，我必须通过添加更多的卷积层来减少形状，还是可以添加一个完全连接的层？我认为它应该有一个完全连接的层，但在每一个教程中，我检查鉴别器没有完全连接的层</p><pre><code>import random\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as torch_dataset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\nseed = 1\nprint(\"Random Seed: \", seed)\nrandom.seed(seed)\ntorch.manual_seed(seed)\nimages_folder_path = \"./spectrograms/\"\n\nbatch_size = 1\nimage_size = 256\nn_channels = 1\nz_vector = 100\nn_features_generator = 32\nn_features_discriminator = 32\nnum_epochs = 5\nlr = 0.0002\nbeta1 = 0.5\n\ndataset = torch_dataset.ImageFolder(\n    root=images_folder_path, transform=transforms.Compose(\n        [\n            transforms.Grayscale(num_output_channels=1),\n            transforms.Resize(image_size),\n            transforms.CenterCrop(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize(0.5, 0.5)\n         ]\n    )\n)\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            nn.ConvTranspose2d(z_vector, n_features_generator * 8, 4, 1, bias=False),\n            nn.BatchNorm2d(n_features_generator * 8),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(n_features_generator * 8, n_features_generator * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(n_features_generator * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(n_features_generator * 4, n_features_generator * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(n_features_generator * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(n_features_generator * 2, n_features_generator, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(n_features_generator),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(n_features_generator, n_channels, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, inputs):\n        return self.main(inputs)\n\n# Convolutional Layer Output Shape = [(W−K+2P)/S]+1\n# W is the input volume\n# K is the Kernel size\n# P is the padding\n# S is the stride\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(n_channels, n_features_discriminator, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(n_features_discriminator, n_features_discriminator * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(n_features_discriminator * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(n_features_discriminator * 2, n_features_discriminator * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(n_features_discriminator * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(n_features_discriminator * 4, n_features_discriminator * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(n_features_discriminator * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(n_features_discriminator * 8, 1, 4, 1, bias=False),\n        )\n\n    def forward(self, inputs):\n        return self.main(inputs)\n\n\nnetG = Generator().to(device)\nif device.type == 'cuda':\n    netG = nn.DataParallel(netG)\nnetG.apply(weights_init)\nprint(netG)\n\nnetD = Discriminator().to(device)\nif device.type == 'cuda':\n    netD = nn.DataParallel(netD)\nnetD.apply(weights_init)\nprint(netD)\n\ncriterion = nn.BCEWithLogitsLoss()\n\nfixed_noise = torch.randn(64, z_vector, 1, 1, device=device)\n\nreal_label = 1.\nfake_label = 0.\n\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n\nimg_list = []\nG_losses = []\nD_losses = []\niters = 0\n\nprint(\"Starting Training Loop...\")\nfor epoch in range(num_epochs):\n    for i, data in enumerate(dataloader, 0):\n        netD.zero_grad()\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n        output = netD(real_cpu)\n        print(output.shape)\n        print(label.shape)\n        output = output.view(-1)\n        errD_real = criterion(output, label)\n        errD_real.backward()\n        D_x = output.mean().item()\n\n        noise = torch.randn(b_size, z_vector, 1, 1, device=device)\n        fake = netG(noise)\n        label.fill_(fake_label)\n        output = netD(fake.detach()).view(-1)\n        errD_fake = criterion(output, label)\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        errD = errD_real + errD_fake\n        optimizerD.step()\n\n        netG.zero_grad()\n        label.fill_(real_label)\n        output = netD(fake).view(-1)\n        errG = criterion(output, label)\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        optimizerG.step()\n\n        if i % 50 == 0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n                  % (epoch, num_epochs, i, len(dataloader),\n                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n\n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise).detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n\n        iters += 1\n</code></pre><p>我得到的错误是：</p><pre><code>Traceback (most recent call last):\n  File \"G:/Pastas Estruturadas/Conhecimento/CEFET/IA/SpectroGAN/dcgan.py\", line 140, in &lt;module&gt;\n    errD_real = criterion(output, label)\n  File \"C:\\Users\\Ramon\\anaconda3\\envs\\vision\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 722, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Users\\Ramon\\anaconda3\\envs\\vision\\lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 631, in forward\n    reduction=self.reduction)\n  File \"C:\\Users\\Ramon\\anaconda3\\envs\\vision\\lib\\site-packages\\torch\\nn\\functional.py\", line 2538, in binary_cross_entropy_with_logits\n    raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\nValueError: Target size (torch.Size([1])) must be the same as input size (torch.Size([169]))\n\n</code></pre><p>输出的形状：<code>torch.Size([1, 1, 13, 13])</code>，标签的形状：<code>torch.Size([1])</code></p></div>"}
{"href": "https://www.cnpython.com/qa/1473159", "title": "对Huggingface Transformers序列分类的predict（）输出感到困惑", "content": "<div class=\"show-content\"><p>下面的大部分代码取自<a href=\"https://huggingface.co/transformers/custom_datasets.html\" rel=\"nofollow noreferrer\">this huggingface doc page, for tensorflow code selections</a>。让我困惑的是，在对几个新句子微调预训练模型并对两个测试集句子运行<code>predict</code>之后，我得到了<code>predict()</code>输出，即16x2数组</p><p>x2是有意义的，因为我有两个类（0,1），但当我通过一个由2个（而不是16个）序列组成的测试集时，为什么长度为16到“SequenceClassification”模型？如何获得两个测试集序列的预测类？（顺便说一句，我从logits转换到预测概率没有问题，只是对输出的形状感到困惑）</p><p>下面是一个可复制的代码示例。也可以在GoogleColab环境<a href=\"https://colab.research.google.com/drive/1pZIRhxlSwuFdPYH2ZuvfL87E58qzGDyy?usp=sharing\" rel=\"nofollow noreferrer\">here</a>中自由地逐步完成代码</p><pre><code>from transformers import DistilBertTokenizerFast\nfrom transformers import TFDistilBertForSequenceClassification\nimport tensorflow as tf\n\n# set up arbitrary example data\ntrain_txt = ['this sentence is about dinosaurs', 'this also mentions dinosaurs', 'this does not']\ntest_txt  = ['the land before time was cool', 'alligators are basically dinosaurs']\ntrain_labels = [1,1,0]\ntest_labels = [1,1]\n\n# convert sentence lists to Distilbert Encodings and then TF Datasets\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\ntrain_encodings = tokenizer([str(s) for s in train_txt], truncation=True, padding=True)\ntest_encodings = tokenizer([str(s) for s in test_txt], truncation=True, padding=True)\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(train_encodings),\n    train_labels\n))\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(test_encodings),\n    test_labels\n))\n\n# Fine-tune pretrained Distilbert Classifier on our data\nmodel = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\nmodel.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn\nmodel.fit(train_dataset.shuffle(1000).batch(3), epochs=3, batch_size=3)\n\n# Generate test-set predictions\ntest_preds = model.predict(test_dataset)\n</code></pre><p><code>test_preds</code>输出：</p><pre><code>&gt;test_preds\nTFSequenceClassifierOutput([('logits', array([[ 0.1527334 ,  0.17010647],\n                                    [ 0.10007463,  0.15664947],\n                                    [-0.10294056,  0.18813357],\n                                    [-0.05231615,  0.1587314 ],\n                                    [-0.11520502,  0.16303074],\n                                    [ 0.00855697,  0.13974288],\n                                    [-0.17962483,  0.12381783],\n                                    [ 0.05765227,  0.04970012],\n                                    [ 0.1527334 ,  0.17010647],\n                                    [-0.12754977,  0.11164709],\n                                    [-0.00847345,  0.12885672],\n                                    [-0.01731028,  0.13520113],\n                                    [-0.08433925,  0.16828224],\n                                    [-0.20086896,  0.08963215],\n                                    [ 0.05765227,  0.04970012],\n                                    [ 0.02467203,  0.15794128]], dtype=float32))])\n</code></pre></div>"}
{"href": "https://www.cnpython.com/qa/1473160", "title": "Python绘图脱机html交互报告", "content": "<div class=\"show-content\"><p>我正在尝试脱机使用html编写一份交互式报告（以便能够通过电子邮件发送）。报告将具有交互式绘图，并附有一些解释（绘图外的文本）。我面临的主要问题是：</p><ul><li>在html中添加选项卡</li><li>选择要添加到hml或选项卡的绘图的大小和位置李&gt;</li><li>向html中添加文本（例如，说明注释）</li></ul><p>我使用plotly而不是bokeh的主要原因是bokeh不允许将列表与脱机html交互绘图中的绘图集成（例如：使用此链接中的列表“流派”）<a href=\"https://demo.bokeh.org/movies\" rel=\"nofollow noreferrer\">https://demo.bokeh.org/movies</a></p><p>plotly v4的文档对于脱机html来说非常稀少。我发现可以将绘图导出为html：</p><pre><code>import plotly.express as px\n\nfig =px.scatter(x=range(10), y=range(10))\nfig.write_html(\"file.html\")\n</code></pre><p>但我仍然无法解决上述三个问题</p><p>有什么建议吗</p></div>"}
{"href": "https://www.cnpython.com/qa/1473162", "title": "如何在python中将字符串对象转换为lambda函数对象", "content": "<div class=\"show-content\"><p>我有一个包含lambda函数的字符串列表，如下所示：</p><pre><code>funcs = [\"lambda x: x * x\", \"lambda x: x + x\"]\n</code></pre><p>我想像使用lambda一样使用这些函数。如果我有这个例子：</p><pre><code>funcs = [lambda x: x * x, lambda x: x + x]\na = funcs[0](3)\nprint(a)\n\n&gt; 9\n</code></pre><p>但首先我需要将这个字符串转换为<code>lambda</code>，将其用作<code>lambda</code>。我该怎么做</p></div>"}
{"href": "https://www.cnpython.com/qa/1473163", "title": "带有MSG_PEEK标志的HTTP客户端recv（）异常行为", "content": "<div class=\"show-content\"><p>我正在编写一个带有python套接字的HTTP客户端</p><p>首先，我尝试只读取足够的字节来获取标题，这样我就可以提取内容长度。请注意，使用了MSG_PEEK标志，它不会从队列（<a href=\"https://manpages.debian.org/buster/manpages-dev/recv.2.en.html\" rel=\"nofollow noreferrer\">stated on this page)</a>）中删除数据</p><pre><code>peek = client.recv(4096, socket.MSG_PEEK).decode(FORMAT)\ncontent_length = get_content_length() # parse headers and get Content-Length\nresponse = client.recv(content_length).decode(FORMAT)\n</code></pre><p>问题是，我没有收到所有的数据（在我的例子中是一个HTML页面），即使缓冲区大小设置为高值，比如100.000，也没有什么区别</p><p>但是，下面的代码确实可以一次检索所有内容。区别在于我没有在这里使用MSG_PEEK</p><pre><code>response = client.recv(100000).decode(FORMAT)\n</code></pre><p>这同样适用于读取传输编码：Chunked。当我使用MSG_PEEK时，只接收块的一部分</p><p>这是正常行为吗？还是应该在循环中调用recv（），直到所有内容都被读取</p></div>"}
{"href": "https://www.cnpython.com/qa/1473164", "title": "通过python将图表从excel复制到powerpoint", "content": "<div class=\"show-content\"><p>我用xlsxwriter创建了一系列图表。你能为我介绍一个代码或者一种方法来将这些图表复制到powerpoint吗</p><p>谢谢</p></div>"}
{"href": "https://www.cnpython.com/qa/1473165", "title": "在Github操作windows msys2上，配置找不到icu", "content": "<div class=\"show-content\"><p>在<code>windows-latest</code>上的Github操作中，我使用以下命令调用脚本：</p><pre><code>C:\\msys64\\msys2_shell.cmd -mingw64 -defterm -here -full-path -no-start -shell bash scripts/cibw_before_all_windows.sh\n</code></pre><p>我不明白所有的标志是什么意思，所以可能是其中一个标志是错误的。<code>cibw_before_all_windows.sh</code>脚本如下所示：</p><pre class=\"lang-sh prettyprint-override\"><code>pacman -S --noconfirm --needed  \\\n           bison  \\\n           flex  \\\n           icu-devel  \\\n           swig\n\nexport CPPFLAGS=\"-I/c/msys2/usr/include ${CPPFLAGS}\"\nexport LDFLAGS=\"-L/c/msys2/usr/lib ${LDFLAGS}\"\nexport PATH=\"/c/msys2/usr/bin/:${PATH}\"\nexport PKG_CONFIG_PATH=\"/c/msys2/usr/lib/pkgconfig:${PKG_CONFIG_PATH}\"\n\n\ncd hfst_src/\nautoreconf -fvi\n./configure --with-unicode-handler=icu\nmake\nmake check V=1 VERBOSE=1\nmake install\ncd ..\n\npython setup.py build_ext\n</code></pre><p>脚本在<code>./configure --with-unicode-handler=icu</code>上失败，输出如下：</p><pre><code>...\n  checking for ld used by g++... C:/msys64/mingw64/x86_64-w64-mingw32/bin/ld.exe\n  checking if the linker (C:/msys64/mingw64/x86_64-w64-mingw32/bin/ld.exe) is GNU ld... yes\n  checking whether the g++ linker (C:/msys64/mingw64/x86_64-w64-mingw32/bin/ld.exe) supports shared libraries... yes\n  checking for g++ option to produce PIC... -DDLL_EXPORT -DPIC\n  checking if g++ PIC flag -DDLL_EXPORT -DPIC works... yes\n  checking if g++ static flag -static works... yes\n  checking if g++ supports -c -o file.o... yes\n  checking if g++ supports -c -o file.o... (cached) yes\n  checking whether the g++ linker (C:/msys64/mingw64/x86_64-w64-mingw32/bin/ld.exe) supports shared libraries... yes\n  checking dynamic linker characteristics... Win32 ld.exe\n  checking how to hardcode library paths into programs... immediate\n  checking for gawk... (cached) gawk\n  checking how to run the C preprocessor... gcc -E\n  checking for bison... bison -y\n  checking for flex... flex\n  checking for lex output file root... lex.yy\n  checking for lex library... none needed\n  checking for library containing yywrap... no\n  checking whether yytext is a pointer... yes\n  checking for getopts... false\n  checking for a Python interpreter with version &gt;= 3.0... python\n  checking for python... /mingw64/bin/python\n  checking for python version... 3.8\n  checking for python platform... win32\n  checking for python script directory... ${prefix}/lib/python3.8/site-packages\n  checking for python extension module directory... ${exec_prefix}/lib/python3.8/site-packages\n  configure: WARNING: Building hfst successfully requires flex newer than 2.5.33 on some platforms. Unless you are building with pre-flex-generated sources, building will probably fail.\n  checking whether ln -s works... no, using cp -pR\n  checking whether make sets $(MAKE)... (cached) yes\n  checking for doxygen... no\n  checking for main in -ldl... no\n  checking for main in -lpthread... yes\n  checking for main in -lm... yes\n  checking for ncurses.h... no\n  checking for curses.h... no\n  checking for termcap.h... yes\n  checking for tgetent in -ltermcap... yes\n  checking for icu-config... /usr/bin/icu-config\n  checking for pkg-config... /mingw64/bin/pkg-config\n  ### icu-config: Can't find /usr/lib/msys-icuuc68.dll - ICU prefix is wrong.\n  ###      Try the --prefix= option \n  ###      or --detect-prefix\n  ###      (If you want to disable this check, use  the --noverify option)\n  ### icu-config: Exitting.\n  expr: syntax error: unexpected argument '50'\n  configure: error: in `/d/a/hfst-python/hfst-python/hfst_src':\n  configure: error: --with-unicode-handler=icu requested but icu&gt;=50 not found\n  See `config.log' for more details\n  Error: Command C:\\msys64\\msys2_shell.cmd -mingw64 -defterm -here -full-path -no-start -shell bash scripts/cibw_before_all_windows.sh failed with code 1. None\n</code></pre><p>从输出中，似乎<code>configure</code>找不到我的新安装的<code>flex</code>、<code>bison</code>和<code>icu</code>。如何让<code>configure</code>从<code>pacman</code>查看安装？总的来说，有没有更好的方法来解决这个问题</p><h2>编辑</h2><p>由<code>pacman</code>安装的版本是</p><pre><code>bison-3.7.4-1\nflex-2.6.4-1\nicu-devel-68.2-1\nswig-4.0.2-1\n</code></pre><p>我说它没有看到我的{<cd5>}和{<cd6>}的原因是它抱怨{<cd13>}。由于pacman的<code>flex</code>版本是<code>flex-2.6.4-1</code>，因此我假设configure在系统的其他地方发现了不同的版本。这使我怀疑它也没有找到任何其他依赖项的<code>pacman</code>版本</cd13></cd6></cd5></p></div>"}
